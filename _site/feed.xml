<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-02T12:00:01-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">jstoetz</title><subtitle>Adventures in R programming and data science for stocks, ETFs and options, as well as commercial real estate and sports gambling.
</subtitle><author><name>Jake Stoetzner</name><email>Jake.stoetzner@gmail.com</email></author><entry><title type="html">Random Everything - Testing What is Really Important</title><link href="http://localhost:4000/r/random%20walk/2020/09/28/Random-Everything/" rel="alternate" type="text/html" title="Random Everything - Testing What is Really Important" /><published>2020-09-28T00:00:00-05:00</published><updated>2020-09-28T00:00:00-05:00</updated><id>http://localhost:4000/r/random%20walk/2020/09/28/Random-Everything</id><content type="html" xml:base="http://localhost:4000/r/random%20walk/2020/09/28/Random-Everything/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;I often hear that the “best traders” can take a random entry and profit by simply managing the trade and applying appropriate account management principles.  Through Monte Carlo analysis and other statistical tests applied to the random walk hypothesis, synthetic and actual stock data for stocks across the S&amp;amp;P 500, I examine whether this is true or not for a limited number of post-entry management techniques, including random exits, profit-targets, stop-losses and trailing-stops.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;toc&quot;&gt;&lt;a name=&quot;toc&quot;&gt;&lt;/a&gt;TOC&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#h1&quot;&gt;Research: Randomness Be Thy Name&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#h1.1&quot;&gt;Types of Entry and Exit&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h1.2&quot;&gt;A Non-Random Fight Over Randomness&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h1.3&quot;&gt;Research Framework&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#h2&quot;&gt;Coin-Flip Stock Tests&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#h2.1&quot;&gt;Test 1: Random Entry/Random Exit for Coin-Flip Stock&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h2.2&quot;&gt;Test 2: Random Entry/Stop-Loss or Profit Target Exit for Coin-Flip Stock&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h2.3&quot;&gt;Test 3: Random Entry/Trailing Stop (%) Exit for Coin-Flip Stock&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h2.4&quot;&gt;Test 4: Random Entry/Trailing Stop (%) or Profit-Target(%) Exit for Coin-Flip Stock&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#h3&quot;&gt;Actual Stock Data Tests&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#h3.1&quot;&gt;Test 1: Random Entry/Random Exit for Actual Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h3.2&quot;&gt;Test 2: Random Entry/Stop-Loss or Profit Target Exit for Actual Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h3.3&quot;&gt;Test 3: Random Entry/Trailing Stop (%) Exit for Actual Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h3.4&quot;&gt;Test 4: Random Entry/Trailing Stop (%) or Profit-Target(%) Exit for Actual Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#h3.5&quot;&gt;What now, Donny?  A Summary of Actual Data Testing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#h4&quot;&gt;Other Tests and Research&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#h4.1&quot;&gt;Momentum, Mean Reversion, Runs and Random-Walk Theory&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#h5&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#h6&quot;&gt;Notes &amp;amp; Research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research-randomness-be-thy-name&quot;&gt;&lt;a name=&quot;h1&quot;&gt;&lt;/a&gt;Research: Randomness Be Thy Name&lt;/h2&gt;
&lt;p&gt;&lt;sub&gt;&lt;a href=&quot;#toc&quot;&gt;jump back to top&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;h3 id=&quot;types-of-entries-and-exits&quot;&gt;&lt;a name=&quot;h1.1&quot;&gt;&lt;/a&gt;Types of Entries and Exits&lt;/h3&gt;
&lt;p&gt;Entering a trade (either by buying to open or selling short to open) or exiting a trade (selling to close or buying to cover to close) means that the trader is initiating a position in a financial instrument.  For ease of reference, “entry” or “entering” a position will be used for buy to open or sell short to open, and “exit” or “exiting” will be used for sell to close or buy to cover.&lt;/p&gt;

&lt;p&gt;To simplify, entries and exits can categorized as follows (Note: I am leaving a lot of entry and exit options out - this is not meant to be an exhaustive list):&lt;/p&gt;

&lt;h4 id=&quot;random&quot;&gt;Random&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:  buy to open/sell short to open at random and sell to close/buy to cover at random&lt;/p&gt;

&lt;h4 id=&quot;time-based&quot;&gt;Time Based&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:  buy to open/sell short to open at 09:30:00 a.m. and sell to close/buy to cover at 10:00:00 a.m.&lt;/p&gt;

&lt;p&gt;The underlying “reason” for the time based entry or exit is inconsequential.  Choosing to enter/exit a position at a given time might be based on a random signal, an indicator or for some other reason.  Regardless, the time is what determines exit and entry.&lt;/p&gt;

&lt;h4 id=&quot;price-based-amt-or--or-indicator&quot;&gt;Price Based (amt or % or indicator)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:  buy to open at $249.&lt;/p&gt;

&lt;h4 id=&quot;volume-based-amt-or--or-indicator&quot;&gt;Volume Based (amt or % or indicator)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:  sell short to open after 5M shares have traded.  Buy to cover if SMA20 of volume crosses below 3.5M shares.&lt;/p&gt;

&lt;h4 id=&quot;stop-or-stop-limit--or--or-indicator&quot;&gt;Stop or Stop-Limit ($ or % or indicator)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: buy to open/sell short to open at $248.50 stop when the stock is trading at $249.00.  The counterpart is to sell to close/buy to cover at $248.50 when you are currenly long or short the stock and the stock is trading at $249.  A stop could also be triggered based on an indicator i.e. buy to open stop $248.50 if SMA 50 greater than SMA 200.&lt;/p&gt;

&lt;p&gt;For this research, a stop and a stop-limit order are the same since we are backtesting the strategy and (based on the data used) can’t distinguish a stop “buy at market” or a stop-limit “buy at x price or better.”&lt;/p&gt;

&lt;h4 id=&quot;profit-target--or--or-indicator&quot;&gt;Profit Target ($ or % or indicator)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: sell to close if profit is equal to $1000.&lt;/p&gt;

&lt;p&gt;Note that profit target must take into account the current position (or non-position)&lt;/p&gt;

&lt;h4 id=&quot;trailing-stop--or--or-indicator&quot;&gt;Trailing Stop ($ or % or indicator)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: exit the position if the open of the next bar is below 1% of the cumulative maximum.&lt;/p&gt;

&lt;p&gt;All of the categories above can &lt;em&gt;all&lt;/em&gt; be combined together to create a trading strategy.  Assuming this is true, that gives 3.556874e+14 possible combinations for just the buy side (and that doesn’t calculate the endless array of paramaters and indicators you could appy).  However, some of these are clearly not useful, and for the sake of keeping things simple, I will look at the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Random Entry/Random Exit&lt;/li&gt;
  &lt;li&gt;Random Entry/Profit Target(%) Exit&lt;/li&gt;
  &lt;li&gt;Random Entry/Trailing Stop(%) Exit&lt;/li&gt;
  &lt;li&gt;Random Entry/Profit Target or Trailing Stop(%) Exit&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;a-non-random-fight-over-randomness&quot;&gt;&lt;a name=&quot;h1.2&quot;&gt;&lt;/a&gt;A Non-Random Fight Over Randomness&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;“In probability theory, a random walk is a stochastic process in which the change in the random variable is uncorrelated with past changes. Hence the change in the random variable cannot be forecasted. For a random walk, there is no pattern to the changes in the random variable, as the existence of any pattern would mean that the changes can be forecasted.” &lt;a href=&quot;https://www.albany.edu/~bd445/Economics_466_Financial_Economics_Slides_Spring_2014/Random_Walk.pdf&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you ever want to start a fight, try to go into a room full of financial advisors and tell them that you can predict the market.  The &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk_hypothesis&quot;&gt;random walk hypothesis&lt;/a&gt; states that stock prices are (you guessed it) random and that they cannot be predicted.  &lt;a href=&quot;https://www.chicagobooth.edu/~/media/34F68FFD9CC04EF1A76901F6C61C0A76.PDF&quot;&gt;Entire industries and careers&lt;/a&gt; are predicated on the fact that you can’t predict the market.  The simplified theory is that any outperformance - &lt;a href=&quot;https://en.wikipedia.org/wiki/Alpha_(finance)&quot;&gt;Alpha&lt;/a&gt; or whatever you call it - is only by chance.&lt;/p&gt;

&lt;p&gt;Conversely, their are also &lt;a href=&quot;https://www.pewresearch.org/2007/11/19/tracking-the-traders/&quot;&gt;entire industries and careers&lt;/a&gt; that are based on the fact that the market is not random and can therefore be predicted by trends, patterrns and historical price and volume action.&lt;/p&gt;

&lt;p&gt;Both sides have their proponents:  “[p]assive investments such as index funds and exchange-traded funds control about 60% of the equity assets, while quantitative funds, those which rely on trend-following models instead of fundamental research from humans, now account for 20% of the market share…”  &lt;a href=&quot;https://www.cnbc.com/2019/06/28/80percent-of-the-stock-market-is-now-on-autopilot.html&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For a short list of interesting articles on the topic, check out the Notes and Research Section at the end of this post.&lt;/p&gt;

&lt;p&gt;Which side is right?  Who cares, really.  The key is &lt;em&gt;you need to decide for yourself&lt;/em&gt;, and what better way to do that than some good ol’ fashioned statistical and Monte Carlo analysis.&lt;/p&gt;

&lt;p&gt;And what if whether or not the stock market is random or not is the wrong answer to be asking?  The sole aim of a trader is to extract as much money from the market as his/her account will allow (I’ll add that such extraction &lt;em&gt;should&lt;/em&gt; be in an “ethical” manner but you define that term and make that decision for yourself).  Why does it matter then if market movement is random or not?  &lt;strong&gt;Something&lt;/strong&gt; makes prices move and you should be agnostic about your belief in &lt;em&gt;why&lt;/em&gt; it happens.  Your only job is to profit off it.  A short passage from &lt;a href=&quot;http://mitp-content-server.mit.edu:18180/books/content/sectbyfn?collid=books_pres_0&amp;amp;id=3909&amp;amp;fn=9780262521161_sch_0001.pdf&quot;&gt;Technical Analysis and Random Walks&lt;/a&gt; struck me as relevant:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The term random has some unfortunate connotations. Random events are often believed to be in some sense “un-caused.” This belief is partly due to misleading comparisons that are sometimes drawn between stock price changes and the behavior of a roulette wheel. The problem is liable to be translated into a philosophical one, but there is nothing mystical or unnatural about the process that generates stock price changes. It is not governed by some frolicsome gremlin.  The random movement of stock prices simply results from competition between a large number of skilled and acquisitive investors.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;research-framework&quot;&gt;&lt;a name=&quot;h1.3&quot;&gt;&lt;/a&gt;Research Framework&lt;/h3&gt;
&lt;p&gt;First, I attempt to answer the question of whether or not an edge is &lt;em&gt;required&lt;/em&gt; to be profitable.&lt;/p&gt;

&lt;p&gt;I will test the 4 random entry and exit rules noted above on a few different “securities.”  In essence, the random entry portion should (hypothetically) remove any “edge” from the equation, thereby allowing me to examine the effects of the post-entry management.  I anticipate that the profit target, trailing stop and indicator may act as the “edge” that will ultimately be profitable.&lt;/p&gt;

&lt;p&gt;The first “security” I test on is a “coin-flip” stock.  The &lt;a href=&quot;https://seekingalpha.com/article/4138835-coin-toss-and-investing-success&quot;&gt;coin-flop stock&lt;/a&gt; is a synthetic security that has a normally distributed (ie, 50-50) chance of increasing or decreasing from one  period to the next. To be somewhat consistent with the market, the coin-flip stock has a lognormal distribution of returns.&lt;/p&gt;

&lt;p&gt;The second security will be actual daily stock data for the S&amp;amp;P 500.  I will mass download the stock data, calculate the periodical returns and use Monte Carlo analysis to see whether the 4 random entry and exit rules are profitable.&lt;/p&gt;

&lt;p&gt;Note that all tests will assume an equal amount of a hypothetical cash account is allocated to each test (ie, no compounding of the account).&lt;/p&gt;

&lt;h2 id=&quot;coin-flip-stock&quot;&gt;&lt;a name=&quot;h2&quot;&gt;&lt;/a&gt;Coin-Flip Stock&lt;/h2&gt;
&lt;p&gt;&lt;sub&gt;&lt;a href=&quot;#toc&quot;&gt;jump back to top&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Assume for a second we have a purely “coin flip” stock.  Whether the stock is positive or negative during a given period is 50-50.  This is a Bernoulli distribution where &lt;a href=&quot;https://www.stat.purdue.edu/~jtroisi/STAT350Spring2015/lectures/discDist.pdf&quot;&gt;one trial of an experiment yields either a success or a failure&lt;/a&gt;.  I will ignore that a stock can remain flat (neither positive nor negative) from one period to the next.  The distribution of returns for the “coin flip stock” is &lt;a href=&quot;https://www.investopedia.com/articles/investing/102014/lognormal-and-normal-distribution.asp&quot;&gt;lognormal&lt;/a&gt; with a mean return of 0 and a standard deviation of 0.005.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set.seed(123)

# - normal distribution of positive or negative
p.sample.signal &amp;lt;- c(-1,1)
p.prob &amp;lt;- 0.5 #percent of times
p.periods &amp;lt;- 200 #number of periods

coin.flips &amp;lt;- sample(p.sample.signal,
                size = p.periods,
                replace = TRUE,
                prob = c(p.prob, 1 - p.prob))

# - lognormal distribution of returns
p.mean.log &amp;lt;- 0
p.sd.log &amp;lt;- .005
coin.returns &amp;lt;- rlnorm(n=p.periods,
                  meanlog = p.mean.log,
                  sdlog = p.sd.log)

# - position
coin.position &amp;lt;- (coin.returns-1)*coin.flips

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;test-1--random-entryrandom-exit-for-coin-flip-stock&quot;&gt;&lt;a name=&quot;h2.1&quot;&gt;&lt;/a&gt;Test 1:  Random Entry/Random Exit for Coin-Flip Stock&lt;/h3&gt;
&lt;p&gt;Now I can build a function that finds the cumulative sum of the returns over a given number of periods.  We can replicate this function hundreds of thousands of times.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.coin.flip &amp;lt;- function(
                  p.prob = 0.5,
                  p.periods = 200,
                  p.mean.log = 0,
                  p.sd.log = .005)
    {
    a &amp;lt;- sample(c(-1,1),p.periods,replace = TRUE, prob = c(p.prob, 1 - p.prob))
    b &amp;lt;- rlnorm(p.periods, p.mean.log, p.sd.log)
    c &amp;lt;- (b-1)*a
    return(c)
  }

fn.final.coin.flip &amp;lt;- function(){
  a &amp;lt;- sum(fn.coin.flip(),na.rm = T)
  return(a)
}

# - find the final value of the coin flip stock 100,000 times
mc.coin.flip &amp;lt;- replicate(100000, fn.final.coin.flip())

# - output a summary of the monte carlo simulation and a histogram
round(summary(mc.coin.flip),5)
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
# -0.30376 -0.04810 -0.00009 -0.00001  0.04829  0.29403

hist(mc.coin.flip,breaks = 100)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results are what is to be expected: (1) 100,000 tests of the cumulative value of the returns after 200 periods averages 0, (2) the max return in all of the 100,000 tests is 29.4% and the minimum return is -30.4%, and (3) the histogram looks roughly normal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/20200928_coin_flip_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The conclusion? If we enter our coin flip stock at random and exit at random, over 100,000 returns, the average is zero.  There is no edge in random entries and random exits for our coin-flip stock.&lt;/p&gt;

&lt;h3 id=&quot;test-2-random-entrystop-loss-or-profit-target-exit-for-coin-flip-stock&quot;&gt;&lt;a name=&quot;h2.2&quot;&gt;&lt;/a&gt;Test 2: Random Entry/Stop-Loss or Profit Target Exit for Coin-Flip Stock&lt;/h3&gt;
&lt;p&gt;The second test is to enter the coin-flip stock at random and then exit if either a profit target or a stop-loss is reached (whichever occurs first).&lt;/p&gt;

&lt;p&gt;For the fixed stop-loss and profit target calculation, I will be using a modified function from the &lt;a href=&quot;https://github.com/systematicinvestor/SIT/blob/master/pkg/R/bt.stop.test.r&quot;&gt;Systematic Investor Toolbox&lt;/a&gt;.  Note:  if you are an R trader, stop what you are doing and go directly to his blog.  Do not pass go.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	fn.fixed.stop.fixed.pt &amp;lt;- function(weight, price, tstart, tend, pstop, pprofit) {
		index = tstart : tend
		if(weight &amp;gt; 0){
			temp = price[ index ] &amp;lt; (1 - pstop) * price[ tstart ]

      # profit target
			temp = temp | price[ index ] &amp;gt; (1 + pprofit) * price[ tstart ]

    } else{

			temp = price[ index ] &amp;gt; (1 + pstop) * price[ tstart ]

      # profit target
			temp = temp | price[ index ] &amp;lt; (1 - pprofit) * price[ tstart ]
    }
	}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then put together a function that returns the simulated path of the coin-flip stock and the % return when either the stop-loss or the fixed profit-target fires.  &lt;strong&gt;Notice that the default function reflects a 2:1 ratio of wins to losses.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.fixed.stop.fixed.pt.coin.flip &amp;lt;- function(
                            start.value = 100,
                            p.periods = 200,
                            positive.percent.stop = .01,
                            p.prob = .5,
                            p.profit = .02)
  {
  a &amp;lt;- fn.coin.flip(p.prob)           # - vector of returns at p.prob
  b &amp;lt;- cumprod(c(start.value, a + 1)) # - vector of stock prices based on those returns
  c &amp;lt;- first(which(fn.fixed.stop.fixed.pt(1, b, 1, p.periods, positive.percent.stop, p.profit))) # - find the index of the first instance where the trail stop fires
  c.1 &amp;lt;- ifelse(is.na(c) == TRUE, p.periods, c)
  d &amp;lt;- b[c.1] # - return the value of the index
  e &amp;lt;- d/start.value-1 # - exit over entry
  return(e)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Replicate the function 100,000 times and analyze the results using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;summary()&lt;/code&gt; function.  Plot a histogram of the results.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mc.fixed.stop.fixed.pt.coin.flip &amp;lt;- replicate(100000,fn.fixed.stop.fixed.pt.coin.flip())
round(summary(mc.fixed.stop.fixed.pt.coin.flip),5)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
# -0.02965 -0.01296 -0.01085 -0.00010  0.02122  0.03993
hist(mc.fixed.stop.fixed.pt.coin.flip)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/20200928_fixed_stop_coin_flip_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What about changing the risk to reward ratio?  Say setting the stop-loss at 0.5% and really cranking the profit target up to 5%?  Or taking smaller wins at the risk of larger losses - a 5% stop-loss and a 1% profit target?  I won’t run it but I’ll save you the suspense; over a larger number of trades, the mean is essentially zero.  *But go ahead&lt;/p&gt;

&lt;h3 id=&quot;-test-3-random-entrytrailing-stop--exit-for-coin-flip-stock&quot;&gt;&lt;a name=&quot;h2.3&quot;&gt;&lt;/a&gt; Test 3: Random Entry/Trailing Stop (%) Exit for Coin-Flip Stock&lt;/h3&gt;

&lt;p&gt;What about using a percent trailing stop?  Does that improve returns? It seems intuitive that a trailing-stop(%) would be profitable (“you lose at most 1% AND your upside is unlimited!”).  But does this apply in a random market?&lt;/p&gt;

&lt;p&gt;Like the profit-target and stop-loss function, I am going to use the trailing stop function from the &lt;a href=&quot;https://github.com/systematicinvestor/SIT/blob/master/R/bt.stop.r&quot;&gt;Systematic Investor Toolbox&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# ----- trailing stop: exit trade once price falls below % from max price since start of trade
fn.trailing.stop.long &amp;lt;- function(weight = 1, price, tstart, tend, positive.percent.stop) {
    index = tstart : tend
    if(weight &amp;gt; 0)
        price[ index ] &amp;lt; (1 - positive.percent.stop) * cummax(price[ index ])
    else
        price[ index ] &amp;gt; (1 + positive.percent.stop) * cummin(price[ index ])
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The trailing stop function acts to track the cumulative max over the given number of periods (for my purposes, it will be the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p.periods&lt;/code&gt; argument from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fn.coin.flip()&lt;/code&gt; function).  It compares the cumulative max to the change in the current price.  If the percent change of the current price divided by the cumulative max is &lt;strong&gt;less&lt;/strong&gt; than the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive.percent.stop&lt;/code&gt; then it returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FALSE&lt;/code&gt;.  If the percent change of the current price divided by the cumulative max is &lt;strong&gt;more&lt;/strong&gt; than the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive.percent.stop&lt;/code&gt; then it returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TRUE&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The “random entry” is the first value calculated by our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fn.coin.flip()&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;The “trailing stop (%)” is then the first value of the index of returns that satisfies the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fn.trailing.stop.long&lt;/code&gt; function.  Since the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fn.trailing.stop.long&lt;/code&gt; was created to accept a vector of stock prices, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cumprod()&lt;/code&gt; function combined with &lt;em&gt;any&lt;/em&gt; starting stock value as the index of stock prices to look at.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.trail.stop.coin.flip &amp;lt;- function(
                            start.value = 100,
                            p.periods = 200,
                            positive.percent.stop = .01,
                            p.prob = .5)
  {
  a &amp;lt;- fn.coin.flip(p.prob) # - vector of returns at p.prob
  b &amp;lt;- cumprod(c(start.value, a + 1)) # - vector of stock prices based on those returns
  c &amp;lt;- first(which(fn.trailing.stop.long(1, b, 1, p.periods,positive.percent.stop ))) # - find the index of the first instance where the trail stop fires
  c.1 &amp;lt;- ifelse(is.na(c) == TRUE, p.periods, c)
  d &amp;lt;- b[c.1] # - return the value of the index
  e &amp;lt;- d/start.value-1 # - exit minus entry
  return(e)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fn.trail.stop.coin.flip()&lt;/code&gt; function returns the percentage return for the trade.&lt;/p&gt;

&lt;p&gt;The results aren’t too promising with the trailing stop set at 1%.  Again, the mean is near zero indicating that, on average, we don’t make a huge amount money over a large sample by “letting our profits run and cutting our losses short”.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mc.trail.stop.coin.flip &amp;lt;- replicate(100000,fn.trail.stop.coin.flip())
round(summary(mc.trail.stop.coin.flip),5)
# Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
# -0.27392 -0.04873 -0.00243 -0.00015  0.04605  0.40692
hist(mc.trail.stop.coin.flip,breaks = 100)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/20200928_trail_stop_coin_flip_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So no real difference than random entry/random exit.  Maybe 1% is too tight of a stop?  What about a “looser” stop of 5%?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mc.trail.stop.coin.flip.5 &amp;lt;- replicate(100000,fn.trail.stop.coin.flip(positive.percent.stop = .05))
round(summary(mc.trail.stop.coin.flip.5),5)

# Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
# -0.27392 -0.04873 -0.00243 -0.00015  0.04605  0.40692
hist(mc.trail.stop.coin.flip.5,breaks = 100)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The 5% trailing stop didn’t really improve performance.  It basically stayed the same.&lt;/p&gt;

&lt;h3 id=&quot;test-4-random-entrytrailing-stop--or-profit-target-exit-for-coin-flip-stock&quot;&gt;&lt;a name=&quot;h2.4&quot;&gt;&lt;/a&gt;Test 4: Random Entry/Trailing Stop (%) or Profit-Target(%) Exit for Coin-Flip Stock&lt;/h3&gt;
&lt;p&gt;Not to spoil the surprise, but by now you should know the result of this test before it is run.  The code and results are below.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.trailing.stop.profit.target &amp;lt;- function(weight, price, tstart, tend, positive.percent.stop, p.profit) {
		index = tstart : tend
		if(weight &amp;gt; 0) {
			temp = price[ index ] &amp;lt; (1 - positive.percent.stop) * cummax(price[ index ])

			# profit target
			temp = temp | price[ index ] &amp;gt; (1 + p.profit) * price[ tstart ]
		} else {
			temp = price[ index ] &amp;gt; (1 + positive.percent.stop) * cummin(price[ index ])

			# profit target
			temp = temp | price[ index ] &amp;lt; (1 - p.profit) * price[ tstart ]		
		}
		return( temp )
	}

fn.trail.stop.prof.targ.coin.flip &amp;lt;- function(
                            start.value = 100,
                            p.periods = 200,
                            positive.percent.stop = .01,
                            p.prob = .5,
                            p.profit = .02)
  {
  a &amp;lt;- fn.coin.flip(p.prob) # - vector of returns at p.prob
  b &amp;lt;- cumprod(c(start.value, a + 1)) # - vector of stock prices based on those returns
  c &amp;lt;- first(which(fn.trailing.stop.profit.target(1, b, 1, p.periods,positive.percent.stop, p.profit))) # - find the index of the first instance where the trail stop fires
  c.1 &amp;lt;- ifelse(is.na(c) == TRUE, p.periods, c)
  d &amp;lt;- b[c.1] # - return the value of the index
  e &amp;lt;- d/start.value-1 # - exit minus entry
  return(e)
}

mc.trail.stop.prof.targ.coin.flip &amp;lt;- replicate(100000,fn.trail.stop.prof.targ.coin.flip())
round(summary(mc.trail.stop.prof.targ.coin.flip),5)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
#-0.02863 -0.01087 -0.00501 -0.00001  0.00669  0.04056
hist(mc.trail.stop.prof.targ.coin.flip,breaks = 100)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/20200928_trail_stop_fixed_profit_coin_flip_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusions-for-coin-flip-stocks&quot;&gt;&lt;a name=&quot;h2.5&quot;&gt;&lt;/a&gt;Conclusions for Coin Flip Stocks&lt;/h3&gt;
&lt;p&gt;Coin flip returns are always going to even out to zero over the long term.  That is, if the market is 100% random and never trends and you have no ability to identify when the market may go up so you can enter (ie, have an edge), then there is no advantage to adding a profit-target, stop-loss or trailing-stop loss.  If a market is random and you are successful  with any of these implements, then it can be attributed  to luck.  You just happened to pick the right stretch of numbers.&lt;/p&gt;

&lt;p&gt;But what if you can &lt;em&gt;skew&lt;/em&gt; the coin flip in your favor - say you can guess the correct flip of the coin 53 times out of 100.  What’s the effect on the coin-flip stock then?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mc.trail.stop.coin.flip.skew &amp;lt;- replicate(100000,fn.trail.stop.coin.flip(p.prob = .47))
round(summary(mc.trail.stop.coin.flip.skew),5)

#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
#
-0.26530 -0.04847 -0.00219  0.00035  0.04635  0.33444
sum(unlist(mc.trail.stop.coin.flip.skew))
# [1] 34.67538
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It’s a significant effect - the mean is now .03% per trade - put another way, if you traded 100,000 times, your total return would be 3467% or if you traded 100 times, you would average a total return of 3.47%.  And recall that this is merely with random entries and random exits.&lt;/p&gt;

&lt;p&gt;Having an “edge” then would seem to be important.  &lt;a href=&quot;https://www.smbtraining.com&quot;&gt;SMB Training&lt;/a&gt; put it better than I could (and although the quote below name-checks trailing-stops, I would also add profit-targets and stop-losses to the statement below):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“…[T]he bottom line is if you do not have an edge in the market, you cannot make money. Period. A trailing stop is not an edge in itself (I am aware of the studies in a well-known trading book that show a profitable trading system with random entries and trailing stops. The key is that the markets tested were not random (they were real futures markets), and the exits were not random (they were a good trailing stop system). If markets were random you could not make money trading, and, if you are not convinced of that, you are missing a key piece of the intuition behind understanding price action.” &lt;a href=&quot;https://www.smbtraining.com/blog/finding-an-edge-in-random-markets&quot;&gt;Finding an edge in random markets&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So if the each of the 4 random entry tests don’t add any value to a purely random stock, then the same theory should also apply to actual stock returns if the random walk theory is correct.  There should be no additional return provided by adding the profit target or trailing stop.  “An implication of the efficient market/random-walk theory is that no trading rule will yield an economic profit.”  &lt;a href=&quot;https://www.albany.edu/~bd445/Economics_466_Financial_Economics_Slides_Spring_2014/Testinge_Random-Walk_Theory.pdf_th&quot;&gt;Testing the Random-Walk Theory&lt;/a&gt;.  You can test this by looking at the probability distribution of the profit from a trading rule - if the random walk theory is correct, then the probability of profit should be random.  Stated otherwise, you should have just “bought-and-held” because all of the entries and exits from the trading system were unecessary.&lt;/p&gt;

&lt;h2 id=&quot;testing-on-actual-stock-data&quot;&gt;&lt;a name=&quot;h3&quot;&gt;&lt;/a&gt;Testing on Actual Stock Data&lt;/h2&gt;
&lt;p&gt;&lt;sub&gt;&lt;a href=&quot;#toc&quot;&gt;jump back to top&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;The first step is to download historical data for the last 360 days for all of the stocks in the S&amp;amp;P 500.  To do this, use the excellent &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BatchGetSymbols&lt;/code&gt; package.  It makes &lt;a href=&quot;https://cran.r-project.org/web/packages/BatchGetSymbols/vignettes/BatchGetSymbols-vignette.html&quot;&gt;quick work of downloading, organizing and saving all the data in one big data frame&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;library(BatchGetSymbols)

# set dates
first.date &amp;lt;- Sys.Date() - 360
last.date &amp;lt;- Sys.Date()
freq.data &amp;lt;- 'daily'

# set tickers
df.SP500 &amp;lt;- GetSP500Stocks()
tickers &amp;lt;- df.SP500$Tickers

l.out &amp;lt;- BatchGetSymbols(tickers = tickers,
                         first.date = first.date,
                         last.date = last.date)

l.out.grouped &amp;lt;- l.out$df.tickers %&amp;gt;% group_by(ticker)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;test-1-random-entryrandom-exit-for-actual-stock-data&quot;&gt;&lt;a name=&quot;h3.1&quot;&gt;&lt;/a&gt;Test #1: Random Entry/Random Exit for Actual Stock Data&lt;/h3&gt;
&lt;p&gt;The function is below for entering and exiting at random.&lt;/p&gt;

&lt;p&gt;A few key issues:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Since I am testing individual stocks (and not a portfolio of multiple stocks) I need to group each of the stocks individually when identifying returns. In Test #1 on actual data, I allow the individual to limit it to only entering and exiting the same ticker by setting the argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;same.ticker = TRUE&lt;/code&gt;.  For instance, if you enter one stock long, you must sell the stock before entering another stock.  If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;same.ticker = FALSE&lt;/code&gt; then the function is ignorant of which stock you enter and which stock you exit; they do not have to be the same.  Note that this may provide unrealistic results as it will not exit the prior stock before entering the new one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is also a maximum holding period argument.  The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.periods&lt;/code&gt; argument defines the highest number of periods to hold the stock.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.rand.exit.actual.data &amp;lt;- function(data = l.out$df.tickers,
                                    max.periods = 100,
                                    same.ticker = TRUE)
                                    {
  a &amp;lt;- round(runif(1, min = 1, max = length(data$price.open))) # - random entry index
  b &amp;lt;- round(runif(1, min = a, max = length(data$price.open))) # - random exit index after entry
  c &amp;lt;- first(which(data$ticker[a:b] != lag(data$ticker[a:b],1))) # - where stock data changes
  d &amp;lt;- min(c,max.periods)
  e &amp;lt;- ifelse(same.ticker == TRUE,
        ifelse(data$ticker[a] == data$ticker[b], # - if ticker is the same for both
          sum(data$ret.closing.prices[a:b], na.rm = T), # - sum of all returns
          sum(data$ret.closing.prices[a:(a+d)], na.rm = T)
        ),
        sum(data$ret.closing.prices[a:b], na.rm = T)
      )
  return(e)
}

mc.rand.exit.actual.data &amp;lt;- replicate(100000,fn.rand.exit.actual.data())

round(summary(mc.rand.exit.actual.data),5)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
#-1.64571 -0.09913  0.00626  0.01088  0.13075  1.59938

# - add in a custom function that plots a uniform distribution over the actual histogram
fn.uniform.hist &amp;lt;- function(data,
                    breaks, xlab = &quot;Percent Return&quot;, main = &quot;Frequency of Returns&quot;)
    {
  h &amp;lt;- hist(data, breaks = breaks, density = breaks, col = &quot;gray&quot;, xlab = xlab, main = main)
  xfit &amp;lt;- seq(min(data), max(data), length = breaks)
  yfit &amp;lt;- dnorm(xfit, mean = mean(data), sd = sd(data))
  yfit &amp;lt;- yfit * diff(h$mids[1:2]) * length(data)
  lines(xfit, yfit, col = &quot;black&quot;, lwd = 2)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/20200928_rand_exit_actual_data_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The histogram looks fairly close to a normal distribution - except notice that the frequency is greater in the middle and at both ends (a “fat tail”).  A brief scan of the research on this shows that there have been multiple 3, 4, 5 and 6 sigma events in the stock market.&lt;/p&gt;

&lt;p&gt;Reduce the max holding period argument to 10 (and run the simulation 10k times rather than 100k times):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mc.rand.exit.actual.data.10 &amp;lt;- replicate(10000,fn.rand.exit.actual.data(max.periods = 10))

round(summary(mc.rand.exit.actual.data.10),5)
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
# -1.09355 -0.02947  0.01046  0.00505  0.05168  0.68247

fn.uniform.hist(mc.rand.exit.actual.data.10, breaks = 200, xlab = &quot;Percent Return&quot;, main = &quot;Frequency of Returns - Actual Data | Random Exit | Max Hold 10&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/20200928_rand_exit_10max_actual_data_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This does little to help the mean return.  It actually decreased with a shorter time frame. This may lead you to explore whether longer holding times increase returns (where have I heard that before?).&lt;/p&gt;

&lt;h3 id=&quot;test-2-random-entry-stop-loss--or-profit-target--exit-for-actual-stock-data&quot;&gt;&lt;a name=&quot;h3.2&quot;&gt;&lt;/a&gt;Test #2: Random Entry/ Stop-Loss (%) or Profit-Target (%) Exit for Actual Stock Data&lt;/h3&gt;

&lt;p&gt;Below is the modified code for the random entry and stop-loss percentage or profit-target percentage for actual stock data.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.fixed.stop.fixed.pt.actual.data &amp;lt;- function(data = l.out$df.tickers,
                                    max.periods = 1000,
                                    same.ticker = TRUE,
                                    positive.percent.stop = .025,
                                    p.profit = .05){
  a &amp;lt;- round(runif(1, min = 1, max = length(data$price.open))) # - random entry index

  b &amp;lt;- length(data$price.open) # - end of the data vector

  c &amp;lt;- first(which(data$ticker[a:b] != lag(data$ticker[a:b],1))) # - where stock ticker changes as measured from the random entry

  d &amp;lt;- min(c,max.periods, na.rm = T) # - return length lesser of stock ticker change OR max hold periods

  a.1 &amp;lt;- first(which(fn.fixed.stop.fixed.pt(1, data$price.open, a, (a+d), positive.percent.stop, p.profit ))) # - return index of where stop loss or profit target fired, as measured from random entry to lesser of stock ticker change OR max hold periods

  a.2 &amp;lt;- min((a + a.1), min((a+d), b),na.rm = T) # - min exit index from entry (trail.stop, change of ticker, max.periods, full length of vector)

  #e &amp;lt;- data$price.open[[a.2]]/data$price.open[[a]]-1

  e &amp;lt;- sum(data$ret.closing.prices[a:a.2], na.rm = T) # - sum of all returns

  return(e)
}

# - mc analysis with max.periods at 1000, stop at 2.5% and profit target at 5%
mc.fixed.stop.fixed.pt.actual.data &amp;lt;- replicate(10000,fn.fixed.stop.fixed.pt.actual.data())

round(summary(mc.trail.stop.actual.data),5)

fn.uniform.hist(mc.fixed.stop.fixed.pt.actual.data, breaks = 200, xlab = &quot;Percent Return&quot;, main = &quot;Frequency of Returns - Actual Data | Stop Loss and Profit Target&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/20200928_profit_targ_stop_loss_actual_data_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Profit target and stop loss with a 2:1 ratio didn’t return a significant non-zero mean.  Again, feel free to change the parameters to see if the ultimate results stay the same.&lt;/p&gt;

&lt;h3 id=&quot;test-3-random-entrytrailing-stop-exit-on-actual-stock-data&quot;&gt;&lt;a name=&quot;h3.3&quot;&gt;&lt;/a&gt;Test #3: Random Entry/Trailing Stop Exit on Actual Stock Data&lt;/h3&gt;

&lt;p&gt;A few notes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.periods&lt;/code&gt; argument is now 1000 periods.  The thinking is that the trailing stop should fire well before the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.periods&lt;/code&gt; limit fires.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive.percent.stop&lt;/code&gt; argument is now 10%.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.trail.stop.actual.data &amp;lt;- function(data = l.out$df.tickers,
                                    max.periods = 1000,
                                    same.ticker = TRUE,
                                    positive.percent.stop = .1){
  a &amp;lt;- round(runif(1, min = 1, max = length(data$price.open))) # - random entry index

  b &amp;lt;- length(data$price.open) # - end of the vector

  c &amp;lt;- first(which(data$ticker[a:b] != lag(data$ticker[a:b],1))) # - where stock ticker changes as measured from the random entry

  d &amp;lt;- min(c,max.periods, na.rm = T)

  a.1 &amp;lt;- first(which(fn.trailing.stop.long(1, data$price.open, a, (a+d), positive.percent.stop ))) # - find the index of the first instance where the trail stop fires a measured from rand entry to lesser of where stock ticker changes or the max hold period

  a.2 &amp;lt;- min((a + a.1), min((a+d), b),na.rm = T) # - min exit index (trail.stop, change of ticker, max.periods, full length)

  #e &amp;lt;- data$price.open[[a.2]]/data$price.open[[a]]-1

  e &amp;lt;- sum(data$ret.closing.prices[a:a.2], na.rm = T) # - sum of all returns

  return(e)
}

# - mc analysis 100k runs - with max.periods at 100 and positive.percent stop at 5%
# - to compare with mc.random.exit.actual.data
mc.trail.stop.actual.data.comp &amp;lt;- replicate(100000,fn.trail.stop.actual.data(max.periods = 100, positive.percent.stop = .05))

round(summary(mc.trail.stop.actual.data.comp),5)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
#-0.90035 -0.05301 -0.01184 -0.00114  0.04277  0.88775

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Comparing the trailing stop (%) exit to the purely random exit is pretty telling.  Random exits net slightly more than 1% on average.  Trailing stop exits (at 5%) actually lose money - to the tune of 11 bips on average.  What’s more obvious is the trailing stop (%) exit has a median return of -1.18%.  So, bottom line, trailing stops without an edge in the entry fair no better (and likely significantly worse) than randomly exiting the position.&lt;/p&gt;

&lt;h3 id=&quot;test-4-random-entrytrailing-stop--or-profit-target--exit-on-actual-stock-data&quot;&gt;&lt;a name=&quot;h3.4&quot;&gt;&lt;/a&gt;Test #4: Random Entry/Trailing Stop (%) or Profit-Target (%) Exit on Actual Stock Data&lt;/h3&gt;
&lt;p&gt;The code for the random entry and trailing stop (%) or profit target (%) closely mimicks the previous code for a trailing stop.  Intuitively, it seems like taking a profit at 5% and limiting loses to no more than 2.5% would be more profitable than the trailing stop (%) alone.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fn.trail.stop.profit.target.actual.data &amp;lt;- function(data = l.out$df.tickers,
                                    max.periods = 1000,
                                    same.ticker = TRUE,
                                    positive.percent.stop = .025,
                                    p.profit = .05){
  a &amp;lt;- round(runif(1, min = 1, max = length(data$price.open))) # - random entry index

  b &amp;lt;- length(data$price.open) # - end of the vector

  c &amp;lt;- first(which(data$ticker[a:b] != lag(data$ticker[a:b],1))) # - where stock ticker changes as measured from the random entry

  d &amp;lt;- min(c,max.periods, na.rm = T)

  a.1 &amp;lt;- first(which(fn.trailing.stop.profit.target(1, data$price.open, a, (a+d), positive.percent.stop, p.stop ))) # - find the index of the first instance where the trail stop fires a measured from rand entry to lesser of where stock ticker changes or the max hold period

  a.2 &amp;lt;- min((a + a.1), min((a+d), b),na.rm = T) # - min exit index (trail.stop, change of ticker, max.periods, full length)

  #e &amp;lt;- data$price.open[[a.2]]/data$price.open[[a]]-1

  e &amp;lt;- sum(data$ret.closing.prices[a:a.2], na.rm = T) # - sum of all returns

  return(e)
}

# - mc analysis 100k runs - with max.periods at 100, positive.percent stop at 2.5% and profit target at 5%
# - to compare with mc.random.exit.actual.data
mc.trail.stop.profit.target.actual.data &amp;lt;- replicate(100000,fn.trail.stop.profit.target.actual.data(max.periods = 100))

# round(summary(mc.trail.stop.profit.target.actual.data),5)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
#-0.80554 -0.02529  0.00801  0.00185  0.03080  0.82386
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Again, no advantage over random exits (but slightly better than trailing stop (%) alone)&lt;/p&gt;

&lt;h3 id=&quot;what-now-donny--a-summary-of-actual-data-testing&quot;&gt;&lt;a name=&quot;h3.5&quot;&gt;&lt;/a&gt;What now, Donny?  A Summary of Actual Data Testing&lt;/h3&gt;
&lt;p&gt;None of the exit rules got close to the mean or median return of a random exit.  Disconcerting? Maybe.  Or maybe it proves that you have to have an edge at entry, during the trade and at the exit.  Trade management alone will not get you to the promised land.&lt;/p&gt;

&lt;h2 id=&quot;other-tests-and-research&quot;&gt;&lt;a name=&quot;h4&quot;&gt;&lt;/a&gt;Other Tests And Research&lt;/h2&gt;
&lt;p&gt;&lt;sub&gt;&lt;a href=&quot;#toc&quot;&gt;jump back to top&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;h3 id=&quot;momentum-mean-reversion-runs-and-random-walk-theory&quot;&gt;&lt;a name=&quot;h4.1&quot;&gt;&lt;/a&gt;Momentum, Mean Reversion, Runs and Random-Walk Theory&lt;/h3&gt;

&lt;p&gt;Momentum investors believe that a positive day today will continue with a positive day tomorrow.  Similarly, a negative day today will be followed by a negative day tomorrow.  Put another way, the odds of an up (positive return) day following an up day are higher than a down (negative day) following an up day.&lt;/p&gt;

&lt;p&gt;Random walk gives the odds as even that an up period will be followed by another up period.  Same thing for down days; whether a down day is followed by another down day is 50% probable. (For a minute, ignore flat days to simplify the calculations).  This is because a move in the future is independent of what happened in the past.&lt;/p&gt;

&lt;p&gt;Mean reversion investments are the mirror opposite of momentum days but have the same basic premise that prior trend predicts future movement:  (1) an up day means its more likely to be followed by a down day that returns to the mean, and (2) a down day is more likely to precede an up day.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another simple statistical test is a runs test. For daily data, a run is defined as a sequence of days in which the stock price changes in the same direction. For example, consider the following combination of upward and downward price changes: + + − − + − + − − − + +. A + sign means that the stock price increased, and a − sign means that the stock price decreased. Thus the example has 7 runs, in 12 observations [note: 4 positive runs (2 of length 2 and 2 of length 1) and 3 negative runs]. &lt;a href=&quot;https://www.albany.edu/~bd445/Economics_466_Financial_Economics_Slides_Spring_2014/Testing_the_Random-Walk_Theory.pdf&quot;&gt;Testing the Random-Walk Theory&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Random walk then expects the number of runs to be n/2, where n is the number of observations.&lt;/p&gt;

&lt;p&gt;For the 12 observations noted above, random walk predicts that, on average, there should only be 12/2 = 6 runs total in all of the observations.  The standard deviation of the expected number of runs is then sqrt(n)/2 or sqrt(12)/2=1.73 runs.  Assuming a normal distribution, a &lt;a href=&quot;https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule#:~:text=For%20an%20approximately%20normal%20data,deviations%20account%20for%20about%2099.7%25.&quot;&gt;2-standard-deviation confidence interval&lt;/a&gt; is then about 2.54 runs on the low side and 9.46 runs on the high side. A 3-standard-deviation interval (99% plus confidence) is 0.81 and 11.19, respectively.&lt;/p&gt;

&lt;p&gt;A momentum theory or mean reversion theory is the same when looking at runs. The number of runs should be lower than what random walk expects &lt;strong&gt;IF&lt;/strong&gt; we are to assume that the market trend continues (momentum) or reverts to the mean.&lt;/p&gt;

&lt;p&gt;R provides an easy “runs”function with the &lt;a href=&quot;https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/rle&quot;&gt;rle() function&lt;/a&gt;.  It computes the lengths and values of runs of &lt;em&gt;equal values&lt;/em&gt; in a vector.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# - set values for an up day equal to 1, down day to -1 and a flat day to 0
a &amp;lt;- ifelse(l.out$df.tickers$ret.closing.prices &amp;gt; 0, 1, ifelse(l.out$df.tickers$ret.closing.prices &amp;lt; 0, -1, 0))

# - replace all NA values with 0
a[is.na(a)] &amp;lt;- 0

# - count the streaks of up days, down days and zero days for all of the data
b &amp;lt;- rle(a)

# - return a summary of the runs of up days
summary(b$lengths[b$values == 1])
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#  1.000   1.000   1.000   2.009   3.000  17.000

# - return a summary of the runs of down days
summary(b$lengths[b$values == -1])
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#  1.000   1.000   1.000   1.835   2.000  12.000

# - total runs - positive
length(b$lengths[b$values == 1])
# [1] 31835
length(b$lengths[b$values == -1])
# [1] 31818
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On average, the data will have a run length of 2 positive days and 1.83 down days.  Positive runs (ie, at least 1 up day) happened a total of 31,835 times while negative runs occured 31,818 times (note: these totals are eerily similar).  Runs measurements do not measure the &lt;em&gt;intensity&lt;/em&gt; of the streak of up or down days, only the fact that they happened a certain number of times.  For example, you may have a 5 positive day run that totaled a 7% gain from start to finish and a 1 negative day run that lost 12%.  Runs then don’t deal with the scale (what I like to call “intensity”) of the move.&lt;/p&gt;

&lt;p&gt;You can compare the total number of runs for both positive and negative days.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pos.runs &amp;lt;- unlist(lapply(1:17, function(x)(length(b$lengths[b$values == 1 &amp;amp; b$lengths == x])/length(b$lengths))))

neg.runs &amp;lt;- unlist(lapply(1:17, function(x)(length(b$lengths[b$values == -1 &amp;amp; b$lengths == x])/length(b$lengths))))

tot.runs &amp;lt;- unlist(lapply(1:17, function(x)(length(b$lengths[b$lengths == x])/length(b$lengths))))

run.table &amp;lt;- cbind(
  &quot;Run Length&quot; = c(1:17),
  &quot;Pos Runs - Count as % of All Runs&quot; = round(pos.runs*100,2),
  &quot;Neg Runs - Count as % of All Runs&quot; = round(neg.runs*100,2),
  &quot;Both Pos &amp;amp; Neg Runs - Count as % of All Runs&quot; = round(tot.runs*100,2)
  )

library(knitr)
kable(run.table)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Run Length&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Pos Runs - Count as % of All Runs&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Neg Runs - Count as % of All Runs&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Both Pos &amp;amp; Neg Runs - Count as % of All Runs&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;25.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;26.89&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;53.40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.33&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11.56&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22.91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.61&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.38&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12.98&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.08&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.33&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.61&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.08&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.97&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.67&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.33&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.19&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.52&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.15&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.07&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.07&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.04&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.04&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.02&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.03&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.01&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.02&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.01&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Compare the actual results to the expected results - determine if it is less than the critical value of 1.64 * sqrt(n)/2.  A number of runs less than the critical value means that their is a only a 5% chance that the number of runs occurred by chance (this is roughly a 90% confidence interval).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# - find actual number of observations (removing 0 values)
n &amp;lt;- length(a[a != 0])

# - expected runs from random walk
exp.runs &amp;lt;- n/2

# - actual runs (removing 0 values)
actual.runs &amp;lt;- length(b$lengths[b$value != 0])

# - sd of runs and critical value
sd.exp.runs &amp;lt;- sqrt(n)/2
critical.value &amp;lt;- 1.64*sd.exp.runs

# - solve for the critical value lower limit  
lower.value &amp;lt;- exp.runs - critical.value

&amp;gt; lower.value
[1] 60893.66
&amp;gt; actual.runs
[1] 63653
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The actual number of runs is far greater than the lower value given by the 90% confidence interval.  At least for this data, the number of runs is in line with what was expected by the random walk theory.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a name=&quot;h5&quot;&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;sub&gt;&lt;a href=&quot;#toc&quot;&gt;jump back to top&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;I can comfortably say that profitability by &lt;em&gt;only&lt;/em&gt; managing a trade after entry is likely a losing proposition.  In other words, you need to have a verifiable edge before entry to expect to make money. If nothing else, it re-inforced the rule that the market, as a whole, is likely governed by the random walk theory.  Does this mean that technical analysis is always a losing proposition?  I think not. The tests done above intentionally did not have a verifiable edge at entry.  When a random entry/trailing stop exit strategy was employed on a stock with a skewed probability (the stock goes up 53 times out of 100), profitability was achieved.  Note this was profitable &lt;em&gt;even when the mean log return for the stock was set at zero&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Also - changing the mean log return to 1% and testing the coin flip stock with random entry and random exit yielded a near 0 mean return over 100k simulations.  Interestingly, using the same return and a trailing stop at 10% had a .33% mean over 100k simulations.  This would need additional research but it leads you to think that, possibly, the trailing stop had some value when a positive return was log normally distributed.&lt;/p&gt;

&lt;p&gt;More research is also certainly needed on other topics.  Future improvements and questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to know when you have a quantifiable edge versus a random run of favorable returns.&lt;/li&gt;
  &lt;li&gt;Adding in account and position management rules.  That is, adding in compounding to maximize geometric mean returns for each security.&lt;/li&gt;
  &lt;li&gt;Include the Kelly Criterion and Optimal f to see if either has a meaningful effect.&lt;/li&gt;
  &lt;li&gt;Accept the inevitable conclusion that you can’t improve performance by predicting the direction of the stock market.  Past results do not predict future performance.  They say luck is what happens when preparation meets opportunity.  So is the key to accept that you need to stay in the game long enough to put yourself in a position to make money?  Is leverage and compounding returns the best way to maximize returns (assuming a perma-bull portfolio)?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notes-and-research&quot;&gt;&lt;a name=&quot;h6&quot;&gt;&lt;/a&gt;Notes and Research&lt;/h2&gt;
&lt;p&gt;&lt;sub&gt;&lt;a href=&quot;#toc&quot;&gt;jump back to top&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support for the Random Walk Theory.
    &lt;ul&gt;
      &lt;li&gt;Fama, Eugene F. “The behavior of stock-market prices.” The journal of Business 38.1 (1965): 34-105. &lt;a href=&quot;https://d1wqtxts1xzle7.cloudfront.net/19110451/the_behavior_of_stock-market_prices__fama__1965.pdf?1340047562=&amp;amp;response-content-disposition=inline%3B+filename%3DThe_behavior_of_stock_market_prices.pdf&amp;amp;Expires=1601394260&amp;amp;Signature=L7zNRwuwdl8Ige5xFb2s28xFDL0t1qPR9opMAJM1Gs54rn8tpQygsvCsYrCNNkl7V7nmWnY1Huv9szdS20uyAqiUffGvT7SmwUdEHMnDh~sASbhA7HEMltr0RBn48ZghrmR4f4JO8tJP7kOEMft3k6mZ~dm6XQxntByQywUqlXwIWs8XLH8a7P76f6xMS8EB0waI67Km7n2Lg3yb0cO1gKzE2OtKGHzfQcEMKmYZdSNV4XJZIgcZWu38EWcbbEiSwQJ0IdHcuzOtnvAn2ZJDHge51~D5WIL-JvKQrJZQduuU1DojdAfNGVHoBp~xOgj3g2c1MupLmsOVvmFWMBXjvg__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA&quot;&gt;PDF Link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Fama, Eugene F. “Random walks in stock market prices.” Financial analysts journal 51.1 (1995): 75-80. &lt;a href=&quot;https://s3.eu-central-1.amazonaws.com/z3r2zxopa4uuqpw5a4ju/devriesinvestment/files/Random%20Walk%20in%20Stock%20Market%20Prices.pdf&quot;&gt;PDF Link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Pinches, George E. “The random walk hypothesis and technical analysis.” Financial Analysts Journal 26.2 (1970): 104-110. &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.2469/faj.v26.n2.104?journalCode=ufaj20&quot;&gt;Link&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Support for the Non-Random Walk Theory.
    &lt;ul&gt;
      &lt;li&gt;Lo, Andrew W., and A. Craig MacKinlay. A non-random walk down Wall Street. Princeton University Press, 2011. &lt;a href=&quot;http://kadamaee.ir/payesh/books-tank/17/Lo%20&amp;amp;%20MacKinlay%20-%20A%20Non-Random%20Walk%20Down%20Wall%20Street%20(1999).pdf&quot;&gt;PDF Link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Hood, Donald C., Paul Andreassen, and Stanley Schachter. “II. Random and non-random walks on the New York stock exchange.” Journal of Economic Behavior &amp;amp; Organization 6.4 (1985): 331-338. &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/0167268185900022&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Gup, Benton E. “A note on stock market indicators and stock prices.” Journal of Financial and Quantitative Analysis (1973): 673-682. &lt;a href=&quot;https://www.jstor.org/stable/2329832&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;(“STOCK MARKET PRICES DO NOT FOLLOW RANDOM WALKS: EVIDENCE FROM A SIMPLE SPECIFICATION TEST”)[https://www.nber.org/papers/w2168.pdf]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[PROBABILITIES AND DISTRIBUTIONS&lt;/td&gt;
          &lt;td&gt;R LEARNING MODULES](https://stats.idre.ucla.edu/r/modules/probabilities-and-distributions/)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://rstudio-pubs-static.s3.amazonaws.com/8492_b817c712a5f6456fb4c5932e3d957135.html#/3&quot;&gt;Simulating a Coin Flip in R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Probabilities and Distributions&lt;/td&gt;
          &lt;td&gt;R Learning Modules](https://stats.idre.ucla.edu/r/modules/probabilities-and-distributions/)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/BatchGetSymbols/vignettes/BatchGetSymbols-vignette.html&quot;&gt;Using BatchGetSymbols to download financial data for several tickers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/systematicinvestor/SIT/blob/master/pkg/R/bt.stop.test.r&quot;&gt;SIT/bt.stop.test.r at master · systematicinvestor/SIT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.albany.edu/~bd445/Economics_466_Financial_Economics_Slides_Spring_2014/Testing_the_Random-Walk_Theory.pdf&quot;&gt;Testing the Random-Walk Theory&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.albany.edu/~bd445/Economics_466_Financial_Economics_Slides_Spring_2014/&quot;&gt;Slides, Economics 466, Financial Economics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.albany.edu/~bd445/Economics_466_Financial_Economics_Slides_Spring_2014/Random_Walk.pdf&quot;&gt;Random Walk&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.albany.edu/~bd445/Economics_466_Financial_Economics_Slides_Spring_2014/The_Weakness_of_the_Efficient-Market_Theory.pdf&quot;&gt;The Weakness of the Efficient-Market Theory&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Are Stock Returns Normally Distributed?&lt;/td&gt;
          &lt;td&gt;by Tony Yiu&lt;/td&gt;
          &lt;td&gt;Towards Data Science](https://towardsdatascience.com/are-stock-returns-normally-distributed-e0388d71267e)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://klementoninvesting.substack.com/p/the-distribution-of-stock-market#:~:text=We%20all%20know%20that%20stock,happen%20more%20frequently%20than%20expected&quot;&gt;The distribution of stock market returns - Klement on Investing&lt;/a&gt;.&amp;amp;text=As%20you%20can%20see%2C%20on,the%20normal%20distribution%20relatively%20well.)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.datamentor.io/r-programming/examples/random-number/&quot;&gt;R Program to Generate Random Number from Standard Distributions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule#:~:text=For%20an%20approximately%20normal%20data,deviations%20account%20for%20about%2099.7%25.&quot;&gt;68–95–99.7 rule&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/assignments/r-tut-rle.html&quot;&gt;18.05 R Tutorial: Run Length Encoding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-an-r-dataframe&quot;&gt;How do I replace NA values with zeros in an R dataframe? - Stack Overflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fOptions/fOptions.pdf&quot;&gt;fOptions.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://stock.tradingninja.com/how-to-price-stock-options-using-r/&quot;&gt;How To Price Stock Options Using R? - Stock Trading Ninja&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.macroption.com/converting-implied-volatility-to-daily-move/&quot;&gt;Converting Implied Volatility to Expected Daily Move - Macroption&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://financetrain.com/black-scholes-options-pricing-model-in-r/&quot;&gt;Black Scholes Options Pricing Model in R - Finance Train&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://quant.stackexchange.com/questions/7817/trading-days-or-calendar-days-for-black-scholes-parameters&quot;&gt;Trading days or calendar days for Black-Scholes parameters? - Quantitative Finance Stack Exchange&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[The Coin Toss And Investing Success&lt;/td&gt;
          &lt;td&gt;Seeking Alpha](https://seekingalpha.com/article/4138835-coin-toss-and-investing-success)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jake Stoetzner</name><email>Jake.stoetzner@gmail.com</email></author><category term="r" /><category term="random walk" /><summary type="html">Overview I often hear that the “best traders” can take a random entry and profit by simply managing the trade and applying appropriate account management principles. Through Monte Carlo analysis and other statistical tests applied to the random walk hypothesis, synthetic and actual stock data for stocks across the S&amp;amp;P 500, I examine whether this is true or not for a limited number of post-entry management techniques, including random exits, profit-targets, stop-losses and trailing-stops.</summary></entry><entry><title type="html">Backtesting Triple Miss Stocks in R</title><link href="http://localhost:4000/r/backtesting/2019/11/04/Bespoke-Triple-Play-Analysis/" rel="alternate" type="text/html" title="Backtesting Triple Miss Stocks in R" /><published>2019-11-04T00:00:00-06:00</published><updated>2019-11-04T00:00:00-06:00</updated><id>http://localhost:4000/r/backtesting/2019/11/04/Bespoke-Triple-Play-Analysis</id><content type="html" xml:base="http://localhost:4000/r/backtesting/2019/11/04/Bespoke-Triple-Play-Analysis/">&lt;p&gt;Backtesting possible short positions by examining stocks that miss EPS and revenue estimates and lower forward guidance reveals that the behavior of these &lt;strong&gt;Triple Miss&lt;/strong&gt; stocks offers a profitable opportunity over the short-term.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#goal&quot;&gt;Goal&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#get-the-data&quot;&gt;Get the Data&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#extract-the-stock-symbols-and-dates-of-each-triple-miss&quot;&gt;Extract the Stock Symbols and Dates of Each Triple Miss&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#get-historical-stock-data&quot;&gt;Get Historical Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#filter-out-data-prior-to-triple-miss&quot;&gt;Filter Out Data Prior to Triple Miss&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#examine-the-aggregate-stock-data&quot;&gt;Examine the Aggregate Stock Data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#examine-the-aggregate-stock-data-1--5-and-10-days-after-triple-miss&quot;&gt;Examine the Aggregate Stock Data 1, 5 and 10 Days After Triple Miss&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#notes---research&quot;&gt;Notes &amp;amp; Research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;www.bespokepremium.com&quot;&gt;Bespoke Investment Group&lt;/a&gt; offers a stock screen feature that “allows users to run in-depth earnings report screens and easily filter more than 150,000 individual quarterly reports for US stocks since 2001.”&lt;/p&gt;

&lt;p&gt;They also offer a report showing what they have termed an &lt;strong&gt;Earnings Triple Play&lt;/strong&gt; - that is a stock that “beats consensus analyst EPS estimates, beats revenue estimates, and raises forward guidance.”  The term has become so well known that &lt;a href=&quot;https://www.investopedia.com/terms/t/triple-play.asp&quot;&gt;none other than Investopedia have given Bespoke credit for it.&lt;/a&gt;  These stocks are the best candidates for long-term buy opportunities.&lt;/p&gt;

&lt;p&gt;But is the opposite also true?  Does a stock that &lt;em&gt;misses&lt;/em&gt; consensus analyst EPS estimates, &lt;em&gt;misses&lt;/em&gt; revenue estimates and &lt;em&gt;lowers&lt;/em&gt; forward guidance a terrible long-term buy opportunity and a possible short opportunity?  We can refer to these stocks in this category as a &lt;strong&gt;Triple Miss&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The goal is to quantify whether a Triple Miss makes a good short opportunity starting the day after the announcement.&lt;/p&gt;

&lt;h2 id=&quot;get-the-data&quot;&gt;Get the Data&lt;/h2&gt;
&lt;h3 id=&quot;extract-the-stock-symbols-and-dates-of-each-triple-miss&quot;&gt;Extract the Stock Symbols and Dates of Each Triple Miss&lt;/h3&gt;
&lt;p&gt;Using the aforementioned stock screener, I researched Triple Miss stocks in 2018, and downloaded a .csv file of the data.  Turns out there were 132 Triple Miss instances (with 104 different stocks - some companies were listed more than once!) in 2018.  On an aggregate basis, this was the return:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018.Triple.Miss.Stats.png&quot; alt=&quot;Aggregate Stats of Triple Miss Stocks&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After reading in the data in R, I extracted the symbols and dates of each Triple Miss.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#create a list of symbols that can be read in
symbols &amp;lt;- unlist(temp$ticker)
ticker &amp;lt;- as.character(unlist(temp$ticker))

#get list of dates of triple miss dates
date &amp;lt;- as.Date(unlist(temp$date))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once this was created, I tried to use the quantmod() package to look-up the historical data.  quantmod() is wonderful for looking up individual stocks and analyzing them, but it is a challenge to deal with multiple stocks on varying dates of interest.&lt;/p&gt;

&lt;h3 id=&quot;get-historical-stock-data&quot;&gt;Get Historical Stock Data&lt;/h3&gt;

&lt;p&gt;Enter the batchgetsymbols() package.&lt;/p&gt;

&lt;p&gt;As the &lt;a href=&quot;https://cran.r-project.org/web/packages/BatchGetSymbols/vignettes/BatchGetSymbols-vignette.html&quot;&gt;creator of the package lamented&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In the past I have used function GetSymbols from the CRAN package quantmod in order to download end of day trade data for several stocks in the financial market. The problem in using GetSymbols is that it does not aggregate or clean the financial data for several tickers. In the usage of GetSymbols, each stock will have its own xts object with different column names and this makes it harder to store data from several tickers in a single dataframe.  Package BatchGetSymbols is my solution to this problem. Based on a list of tickers and a time period, BatchGetSymbols will download price data from yahoo finance and organize it so that you don’t need to worry about cleaning it yourself.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The batchgetsymbols() package worked beautifully to download all of the historical data for each stock going back to 2018 and return it in one giant, clean dataframe.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#&amp;lt;-----TRIPLE MISS ANALYSIS -----&amp;gt;
#Batch download symbols - change type.return to &quot;log&quot; for log
l.out   &amp;lt;-  BatchGetSymbols(tickers   = ticker,
                         first.date   = start.data,
                         last.date    = end.data,
                         type.return  = &quot;arit&quot;,
                         freq.data    = freq.data)

#get a list of tickers that were downloaded
all.ticker &amp;lt;- l.out$df.control$ticker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;filter-out-data-prior-to-triple-miss&quot;&gt;Filter Out Data Prior to Triple Miss&lt;/h3&gt;
&lt;p&gt;After getting the dataframe set-up, I used the dplyr() package to filter out data prior to the Triple Miss announcement.  This was required because I only wanted to look at the performance of the stock &lt;em&gt;after&lt;/em&gt; the date of the Triple Miss. I then created a new data frame to hold this data using the bind_rows() function.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#filter out all data before entry date by ticker
b &amp;lt;- mapply(function(a,b)(l.out$df.tickers %&amp;gt;% filter(ref.date &amp;gt; a, ticker == b)),date,ticker,SIMPLIFY = F)

#create new df without the post entry date data
b1 &amp;lt;- bind_rows(b)
summary(b1$ret.closing.prices)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;examine-the-aggregate-stock-data&quot;&gt;Examine the Aggregate Stock Data&lt;/h3&gt;
&lt;p&gt;Interestingly, the average cumulative daily return for each stock measured from the date of the Triple Miss to the date of this post has been 4.28% - that means an average annual return of 1.36%.  The average daily return is .005%.  On an annualized basis, this is a Sharpe Ratio (return/volatility) of 0.59 - clearly a lot of volatility for not a whole lot of return!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ch.All.png&quot; alt=&quot;Individual Triple Miss Stocks Cumulative Return All Days&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;b2 &amp;lt;- b1 %&amp;gt;%
  select(ref.date,ticker,ret.closing.prices) %&amp;gt;%
    group_by(ticker) %&amp;gt;%
      summarise(Mean_Daily_Return = mean(ret.closing.prices,na.rm = TRUE)*100, Cum_Daily_Return = sum(ret.closing.prices,na.rm = TRUE)*100) %&amp;gt;%         arrange(Cum_Daily_Return)

&amp;gt; mean(b2$Cum_Daily_Return)
[1] 4.281108
&amp;gt; mean(b2$Mean_Daily_Return)
[1] 0.00539823
&amp;gt; mean(b2$Mean_Daily_Return)/sd(b2$Mean_Daily_Return)*sqrt(252)
[1] 0.5942328
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;examine-the-aggregate-stock-data-1-5-and-10-days-after-triple-miss&quot;&gt;Examine the Aggregate Stock Data 1, 5 and 10 Days After Triple Miss&lt;/h3&gt;
&lt;p&gt;What if we shortened the window for analyzing the returns of all of the Triple Miss Stocks?  Do the effects of the Triple Miss “wear off” at a certain point or do they continue?&lt;/p&gt;

&lt;p&gt;The Triple Miss stocks are very negative - below is a chart with the cumulative return from 1 to 10 days after each Triple Miss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ch.10days.all.png&quot; alt=&quot;All Triple Miss Stocks Cumulative Return Day 1 to 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On an individual basis, you can see the downward trend each stock exhibits post-Triple Miss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ch.10days.png&quot; alt=&quot;Individual Triple Miss Stocks Cumulative Return Day 1 to 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On an aggregate basis after 10 days:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;71 out of the 113 stocks (approximately 63%) had a negative cumulative daily return 10 days after the Triple Miss,&lt;/li&gt;
  &lt;li&gt;each stock had an average daily return of -0.58% and cumulatively lost 4.55% in value by day 10, and&lt;/li&gt;
  &lt;li&gt;had an average Sharpe Ratio (average daily return divided by average standard deviation of the daily returns) of -5.57.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In short, the returns were &lt;strong&gt;less&lt;/strong&gt; than spectacular.&lt;/p&gt;

&lt;p&gt;Over 5 days, returns were even worse:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;66 out of the 113 stocks (approximately 58%) had a negative cumulative daily return 5 days after the Triple Miss,&lt;/li&gt;
  &lt;li&gt;each stock had an average daily return of -1.46% and cumulatively lost 5.26% in value by day 5, and&lt;/li&gt;
  &lt;li&gt;had an average Sharpe Ratio of -6.50.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ch.5days.all.png&quot; alt=&quot;All Triple Miss Stocks Cumulative Return Day 1 to 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the day after a Triple miss?  95 of 113 (84%) were down day-over-day by an average of 6.86% with a Sharpe Ratio of -10.62.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It seems clear that Triple Miss stocks would be a good place to start when looking for stocks to short over the near-term.  Obviously, this is not a recommendation to buy or sell; you would need to do your own research and make your own decision to determine that.  But the Triple Miss stocks might be a good filter to start looking for opportunities.&lt;/p&gt;

&lt;h2 id=&quot;notes--research&quot;&gt;Notes &amp;amp; Research&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/analyzing-stocks-using-r-550be7f5f20d&quot;&gt;Analyzing Stocks Using R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ntguardian.wordpress.com/2017/03/27/introduction-stock-market-data-r-1/&quot;&gt;An Introduction to Stock Market Data Analysis with R (Part 1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[getSymbols Extra&lt;/td&gt;
          &lt;td&gt;R-bloggers](https://www.r-bloggers.com/getsymbols-extra/)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quantmod.com/examples/data/&quot;&gt;quantmod: examples :: data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmingforfinance.com/2017/10/different-ways-to-obtain-and-manipulate-stock-data-in-r-using-quantmod/&quot;&gt;Different Ways to Obtain and Manipulate Stock Data In R Using quantmod – Programming For Finance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://srdas.github.io/MLBook/MoreDataHandling.html#using-the-apply-class-of-functions&quot;&gt;Data Science: Theories, Models, Algorithms, and Analytics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ02-quant-integrations-in-tidyquant.html#performanceanalytics-functionality&quot;&gt;R Quantitative Analysis Package Integrations in tidyquant&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bookdown.org/sstoeckl/Tidy_Portfoliomanagement_in_R/s-4portfolios.html&quot;&gt;Tidy Portfoliomanagement in R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://rstudio-pubs-static.s3.amazonaws.com/288218_117e183e74964557a5da4fc5902fc671.html#first-order-of-business---basic-manipulations&quot;&gt;Manipulating Time Series Data in R with xts &amp;amp; zoo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1127/slides/chapter_4.pdf&quot;&gt;chapter4.key&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.msperlin.com/pafdR/Financial-data.html&quot;&gt;Processing and Analyzing Financial Data with R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://faculty.washington.edu/ezivot/econ424/Working%20with%20Time%20Series%20Data%20in%20R.pdf&quot;&gt;Working with Time Series Data in R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://statmath.wu.ac.at/~hornik/QFS1/quantmod-vignette.pdf&quot;&gt;quantmod-vignette.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/26640795/using-lapply-on-quantmod-get-straight-to-xts-object&quot;&gt;r - Using lapply on quantmod, get straight to xts object? - Stack Overflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/BatchGetSymbols/vignettes/BatchGetSymbols-vignette.html&quot;&gt;Using BatchGetSymbols to download financial data for several tickers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Easy multi-panel plots in R using facet_wrap() and facet_grid() from ggplot2&lt;/td&gt;
          &lt;td&gt;Technical Tidbits From Spatial Analysis &amp;amp; Data Science](http://zevross.com/blog/2019/04/02/easy-multi-panel-plots-in-r-using-facet_wrap-and-facet_grid-from-ggplot2/)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sthda.com/english/articles/17-tips-tricks/57-dplyr-how-to-add-cumulative-sums-by-groups-into-a-data-framee/&quot;&gt;dplyr: How to Add Cumulative Sums by Groups Into a Data Frame? - Articles - STHDA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Make Beautiful Tables with the Formattable Package&lt;/td&gt;
          &lt;td&gt;R-bloggers](https://www.r-bloggers.com/make-beautiful-tables-with-the-formattable-package/)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization&quot;&gt;ggplot2 barplots : Quick start guide - R software and data visualization - Easy Guides - Wiki - STHDA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Make Beautiful Tables with the Formattable Package&lt;/td&gt;
          &lt;td&gt;Displayr](https://www.displayr.com/formattable/)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jake Stoetzner</name><email>Jake.stoetzner@gmail.com</email></author><category term="r" /><category term="backtesting" /><category term="Bespoke" /><summary type="html">Backtesting possible short positions by examining stocks that miss EPS and revenue estimates and lower forward guidance reveals that the behavior of these Triple Miss stocks offers a profitable opportunity over the short-term. Goal Get the Data Extract the Stock Symbols and Dates of Each Triple Miss Get Historical Stock Data Filter Out Data Prior to Triple Miss Examine the Aggregate Stock Data Examine the Aggregate Stock Data 1, 5 and 10 Days After Triple Miss Conclusion Notes &amp;amp; Research Goal Bespoke Investment Group offers a stock screen feature that “allows users to run in-depth earnings report screens and easily filter more than 150,000 individual quarterly reports for US stocks since 2001.” They also offer a report showing what they have termed an Earnings Triple Play - that is a stock that “beats consensus analyst EPS estimates, beats revenue estimates, and raises forward guidance.” The term has become so well known that none other than Investopedia have given Bespoke credit for it. These stocks are the best candidates for long-term buy opportunities. But is the opposite also true? Does a stock that misses consensus analyst EPS estimates, misses revenue estimates and lowers forward guidance a terrible long-term buy opportunity and a possible short opportunity? We can refer to these stocks in this category as a Triple Miss. The goal is to quantify whether a Triple Miss makes a good short opportunity starting the day after the announcement. Get the Data Extract the Stock Symbols and Dates of Each Triple Miss Using the aforementioned stock screener, I researched Triple Miss stocks in 2018, and downloaded a .csv file of the data. Turns out there were 132 Triple Miss instances (with 104 different stocks - some companies were listed more than once!) in 2018. On an aggregate basis, this was the return: After reading in the data in R, I extracted the symbols and dates of each Triple Miss. #create a list of symbols that can be read in symbols &amp;lt;- unlist(temp$ticker) ticker &amp;lt;- as.character(unlist(temp$ticker)) #get list of dates of triple miss dates date &amp;lt;- as.Date(unlist(temp$date)) Once this was created, I tried to use the quantmod() package to look-up the historical data. quantmod() is wonderful for looking up individual stocks and analyzing them, but it is a challenge to deal with multiple stocks on varying dates of interest. Get Historical Stock Data Enter the batchgetsymbols() package. As the creator of the package lamented: In the past I have used function GetSymbols from the CRAN package quantmod in order to download end of day trade data for several stocks in the financial market. The problem in using GetSymbols is that it does not aggregate or clean the financial data for several tickers. In the usage of GetSymbols, each stock will have its own xts object with different column names and this makes it harder to store data from several tickers in a single dataframe. Package BatchGetSymbols is my solution to this problem. Based on a list of tickers and a time period, BatchGetSymbols will download price data from yahoo finance and organize it so that you don’t need to worry about cleaning it yourself. The batchgetsymbols() package worked beautifully to download all of the historical data for each stock going back to 2018 and return it in one giant, clean dataframe. #&amp;lt;-----TRIPLE MISS ANALYSIS -----&amp;gt; #Batch download symbols - change type.return to &quot;log&quot; for log l.out &amp;lt;- BatchGetSymbols(tickers = ticker, first.date = start.data, last.date = end.data, type.return = &quot;arit&quot;, freq.data = freq.data) #get a list of tickers that were downloaded all.ticker &amp;lt;- l.out$df.control$ticker Filter Out Data Prior to Triple Miss After getting the dataframe set-up, I used the dplyr() package to filter out data prior to the Triple Miss announcement. This was required because I only wanted to look at the performance of the stock after the date of the Triple Miss. I then created a new data frame to hold this data using the bind_rows() function. #filter out all data before entry date by ticker b &amp;lt;- mapply(function(a,b)(l.out$df.tickers %&amp;gt;% filter(ref.date &amp;gt; a, ticker == b)),date,ticker,SIMPLIFY = F) #create new df without the post entry date data b1 &amp;lt;- bind_rows(b) summary(b1$ret.closing.prices) Examine the Aggregate Stock Data Interestingly, the average cumulative daily return for each stock measured from the date of the Triple Miss to the date of this post has been 4.28% - that means an average annual return of 1.36%. The average daily return is .005%. On an annualized basis, this is a Sharpe Ratio (return/volatility) of 0.59 - clearly a lot of volatility for not a whole lot of return! b2 &amp;lt;- b1 %&amp;gt;% select(ref.date,ticker,ret.closing.prices) %&amp;gt;% group_by(ticker) %&amp;gt;% summarise(Mean_Daily_Return = mean(ret.closing.prices,na.rm = TRUE)*100, Cum_Daily_Return = sum(ret.closing.prices,na.rm = TRUE)*100) %&amp;gt;% arrange(Cum_Daily_Return) &amp;gt; mean(b2$Cum_Daily_Return) [1] 4.281108 &amp;gt; mean(b2$Mean_Daily_Return) [1] 0.00539823 &amp;gt; mean(b2$Mean_Daily_Return)/sd(b2$Mean_Daily_Return)*sqrt(252) [1] 0.5942328 Examine the Aggregate Stock Data 1, 5 and 10 Days After Triple Miss What if we shortened the window for analyzing the returns of all of the Triple Miss Stocks? Do the effects of the Triple Miss “wear off” at a certain point or do they continue? The Triple Miss stocks are very negative - below is a chart with the cumulative return from 1 to 10 days after each Triple Miss. On an individual basis, you can see the downward trend each stock exhibits post-Triple Miss. On an aggregate basis after 10 days: 71 out of the 113 stocks (approximately 63%) had a negative cumulative daily return 10 days after the Triple Miss, each stock had an average daily return of -0.58% and cumulatively lost 4.55% in value by day 10, and had an average Sharpe Ratio (average daily return divided by average standard deviation of the daily returns) of -5.57. In short, the returns were less than spectacular. Over 5 days, returns were even worse: 66 out of the 113 stocks (approximately 58%) had a negative cumulative daily return 5 days after the Triple Miss, each stock had an average daily return of -1.46% and cumulatively lost 5.26% in value by day 5, and had an average Sharpe Ratio of -6.50. And the day after a Triple miss? 95 of 113 (84%) were down day-over-day by an average of 6.86% with a Sharpe Ratio of -10.62. Conclusion It seems clear that Triple Miss stocks would be a good place to start when looking for stocks to short over the near-term. Obviously, this is not a recommendation to buy or sell; you would need to do your own research and make your own decision to determine that. But the Triple Miss stocks might be a good filter to start looking for opportunities. Notes &amp;amp; Research Analyzing Stocks Using R An Introduction to Stock Market Data Analysis with R (Part 1) [getSymbols Extra R-bloggers](https://www.r-bloggers.com/getsymbols-extra/) quantmod: examples :: data Different Ways to Obtain and Manipulate Stock Data In R Using quantmod – Programming For Finance Data Science: Theories, Models, Algorithms, and Analytics R Quantitative Analysis Package Integrations in tidyquant Tidy Portfoliomanagement in R Manipulating Time Series Data in R with xts &amp;amp; zoo chapter4.key Processing and Analyzing Financial Data with R Working with Time Series Data in R quantmod-vignette.pdf r - Using lapply on quantmod, get straight to xts object? - Stack Overflow Using BatchGetSymbols to download financial data for several tickers [Easy multi-panel plots in R using facet_wrap() and facet_grid() from ggplot2 Technical Tidbits From Spatial Analysis &amp;amp; Data Science](http://zevross.com/blog/2019/04/02/easy-multi-panel-plots-in-r-using-facet_wrap-and-facet_grid-from-ggplot2/) dplyr: How to Add Cumulative Sums by Groups Into a Data Frame? - Articles - STHDA [Make Beautiful Tables with the Formattable Package R-bloggers](https://www.r-bloggers.com/make-beautiful-tables-with-the-formattable-package/) ggplot2 barplots : Quick start guide - R software and data visualization - Easy Guides - Wiki - STHDA [Make Beautiful Tables with the Formattable Package Displayr](https://www.displayr.com/formattable/)</summary></entry><entry><title type="html">Recreating a 1976 Least Squares Betting System in R</title><link href="http://localhost:4000/r/sports-betting/2019/10/24/Recreating-a-1976-Least-Squares-Betting-System-in-R/" rel="alternate" type="text/html" title="Recreating a 1976 Least Squares Betting System in R" /><published>2019-10-24T00:00:00-05:00</published><updated>2019-10-24T00:00:00-05:00</updated><id>http://localhost:4000/r/sports-betting/2019/10/24/Recreating-a-1976-Least-Squares-Betting-System-in-R</id><content type="html" xml:base="http://localhost:4000/r/sports-betting/2019/10/24/Recreating-a-1976-Least-Squares-Betting-System-in-R/">&lt;p&gt;Creating a sports betting system that beats the bookmaker is, to say the least, a challenge.  In this post, I create a betting system using least squares regression that has an accuracy rate of up to 70% using out-of-sample data.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;table-contents&quot;&gt;Table Contents&lt;/h2&gt;
&lt;!-- TOC depthFrom:2 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#table-contents&quot;&gt;Table Contents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#research&quot;&gt;Research&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#implied-probability-and-betting-odds-decimal-fraction-and-american-style&quot;&gt;Implied Probability and Betting Odds - Decimal, Fraction and American Style&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#implied-probability-and-system-value&quot;&gt;Implied Probability and System Value&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#converting-an-american-money-line-to-implied-probability&quot;&gt;Converting an American Money Line to Implied Probability&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bookmakers-margin&quot;&gt;Bookmaker’s Margin&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#least-squares-regression&quot;&gt;Least Squares Regression&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#basics-of-least-squares-regression&quot;&gt;Basics of Least Squares Regression&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mathematics-of-least-squares-regression&quot;&gt;Mathematics of Least Squares Regression&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#r-squared&quot;&gt;R Squared&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#using-r-to-model-money-line-minimums&quot;&gt;Using R to Model Money Line Minimums&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#modeling-nfl-money-line-implied-probability-minimums&quot;&gt;Modeling NFL Money Line Implied Probability Minimums&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#locating-and-preparing-the-money-line-data&quot;&gt;Locating and Preparing the Money Line Data&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#calculating-average-implied-probability-and-expected-value-for-nfl-favorites-and-underdogs&quot;&gt;Calculating Average Implied Probability and Expected Value for NFL Favorites and Underdogs&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#solving-for-the-required-system-winning-percentage-based-on-the-implied-money-line-odds&quot;&gt;Solving for the Required System Winning Percentage based on the Implied Money Line Odds&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#testing&quot;&gt;Testing&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#an-overview-of-the-1976-system&quot;&gt;An Overview of the 1976 System&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-compres-and-comperank-packages&quot;&gt;The compres and comperank Packages&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-preparation-in-sample-and-out-of-sample-division&quot;&gt;Data Preparation: In-Sample and Out-Of Sample Division&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#simple-linear-regression&quot;&gt;Simple Linear Regression&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#multiple-regression&quot;&gt;Multiple Regression&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sources-and-notes&quot;&gt;Sources and Notes&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#overview-of-predictive-systems-for-sports&quot;&gt;Overview of Predictive Systems for Sports&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#odds-implied-probability-and-margin&quot;&gt;Odds, Implied Probability and Margin&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#least-squares-and-regression&quot;&gt;Least Squares and Regression&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-comperes-comprank-and-playerratings-packages&quot;&gt;The comperes, comprank and PlayerRatings Packages&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research&quot;&gt;Research&lt;/h2&gt;

&lt;h3 id=&quot;implied-probability-and-betting-odds---decimal-fraction-and-american-style&quot;&gt;Implied Probability and Betting Odds - Decimal, Fraction and American Style&lt;/h3&gt;

&lt;h4 id=&quot;implied-probability-and-system-value&quot;&gt;Implied Probability and System Value&lt;/h4&gt;
&lt;p&gt;“Implied probability is a conversion of betting odds into a percentage. It takes into account the bookmaker margin to express the expected probability of an outcome occurring….if the implied probability is less than your assessment, then it represents betting value.” See &lt;a href=&quot;https://help.smarkets.com/hc/en-gb/articles/214058369-How-to-calculate-implied-probability-in-betting&quot;&gt;How to calculate implied probability in betting.&lt;/a&gt;  Most betting systems seek to find &lt;em&gt;value&lt;/em&gt; by identifying a difference between the implied probability of the bookmaker’s odds and the system’s odds.  For instance, a bookmaker may be offering even (50%) odds that Team A will win versus Team B, but your system indicates Team A is a 60% favorite to win.  Value then is found by betting Team A.&lt;/p&gt;

&lt;h4 id=&quot;converting-an-american-money-line-to-implied-probability&quot;&gt;Converting an American Money Line to Implied Probability&lt;/h4&gt;
&lt;p&gt;A Money Line bet is a bet that a specific team will win. American based money lines can be positive or negative. Negative odds mean that the team or player is favored.  Positive odds mean that the team or player is the underdog.  American Money Lines are generally in multiples of $100 and indicate the amount of money you would need to risk to win the indicated amount.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;American Odds&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Example&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Negative Money Line&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Favorite&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-600 = bet $600 to win $100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Positive Money Line&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Underdog&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;+450 = bet $100 to win $450&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A negative money line means that team is favored; a line of -600 for Team A means that the bookmaker believes Team A will win 6 out of 7 times (expressed as a fraction as 1/6 or as a decimal as 1.167).&lt;/p&gt;

&lt;p&gt;For negative American Odds, implied probability can be calculated as:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Negative American odds / (Negative American odds + 100) * 100 = Implied Probability
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note that you use the absolute value of the negative odds when solving for Implied Probability.&lt;/p&gt;

&lt;p&gt;Conversely, a positive money line means the team is the underdog. A line of +450 (9/2 as a fraction and 5.5 as a decimal) means that Team B would only win 2 out of every 11 matches. Implied probability for positive money lines are calculated by:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;100 / (positive American odds + 100) * 100 = implied probability
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For our example of Team A at -600 and Team B at +450, the implied probability for each team winning is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Team A = 600/(600+100)*100 = 85.7%

Team B = 450/(450+100)*100 = 18.1%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;bookmakers-margin&quot;&gt;Bookmaker’s Margin&lt;/h4&gt;
&lt;p&gt;Note the two implied probabilities for team A and team B do not add up to 100%. This is common as bookmakers add in &lt;em&gt;margin&lt;/em&gt; for each bet. This margin (also called “vig”) is the profit the bookmaker can expect to make assuming that there is equal action on each side of the bet. The margin can be calculated by:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Margin = (Implied Probability Team A + Implied Probability Team B)-1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the margin for our example for Team A and Team B is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Margin = (85.7% + 18.1%)-1 = 3.81%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Margin can also be calculated from decimal odds by:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(1/Decimal Odds option A) * 100 + (1/Decimal Odds option B) * 100 = Margin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;least-squares-regression&quot;&gt;Least Squares Regression&lt;/h3&gt;

&lt;h4 id=&quot;basics-of-least-squares-regression&quot;&gt;Basics of Least Squares Regression&lt;/h4&gt;
&lt;p&gt;Imagine that you are invited to play a round of golf with Tiger Woods. Tiger is, to say the least, a golfer that is far better than average. On the other hand, you are &lt;em&gt;far&lt;/em&gt; below average at best and play once or twice a year. On the agreed date, it’s you and Tiger on the first tee. Tiger steps up and hammers one 350 yards down the middle. You tee up your ball, calm your nerves and whack one out there 225 yards as straight as an arrow! You repeat this feat on the 2nd and 3rd holes. Tiger even comments on how well you are playing. On the 4th hole, your confidence is high as you step up to the tee box, driver in hand. If you had to guess, where do you think your drive will end up?&lt;/p&gt;

&lt;p&gt;If you’re like most golfers, that next shot will likely be a shank into the rough!  You outpaced your ability on the first three holes. It was only a matter of time before you got back to how you really play golf - terribly! The shank off the tee is your golfing ability reverting to your average.&lt;/p&gt;

&lt;p&gt;In statistical terms, a linear regression is based on a reversion to the mean.  Just like a bad golfer shanking it off the tee after a few good shots, regression works on the theory that most things in life will return to normal.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When you hear the word regression, you probably think of how extreme performance during an earlier period most likely gets closer to average during a later period. It’s difficult to sustain an outlier performance. &lt;a href=&quot;https://thepowerrank.com/2018/08/14/how-to-make-accurate-football-predictions-with-linear-regression/&quot;&gt;ThePowerRank.com.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal of any forecasting or predictive system is as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How does a quantity in an earlier period predict the same quantity during a later period? Some quantities persist from the early to later period, which makes a prediction possible. For other quantities, measurements during the earlier period have no relationship to the later period. You might as well guess the mean, which corresponds to our intuitive idea of regression. &lt;a href=&quot;https://thepowerrank.com/2018/08/14/how-to-make-accurate-football-predictions-with-linear-regression/&quot;&gt;ThePowerRank.com.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;mathematics-of-least-squares-regression&quot;&gt;Mathematics of Least Squares Regression&lt;/h4&gt;
&lt;p&gt;Least squares regression seeks to minimize the error between a line of best fit and the measured data points on an x- and y-coordinate graph. Dependent variables are illustrated on the vertical y-axis, while independent variables are illustrated on the horizontal x-axis.  In linear regression, a line of best fit is generated by using the equation of a line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = mx + b

Where:

- m = slope of the line
- b = y-intercept
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://www.mathsisfun.com/data/images/least-squares2.svg&quot; alt=&quot;Example Line of Best Fit from MathIsFun.com&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you square the errors and add them all together, the line with the “minimum” or least total is the line of best fit. Hence, the “least squares error.”&lt;/p&gt;

&lt;h4 id=&quot;r-squared&quot;&gt;R Squared&lt;/h4&gt;
&lt;p&gt;Our hypothesis is that future period data can be predicted based on past period data. Later data then is &lt;em&gt;dependent&lt;/em&gt; on our past, &lt;em&gt;independent&lt;/em&gt; variable. The past data could be anything - margin of victory, relative strength, turnovers etc. We can then measure how well that independent data predicts future period data.&lt;/p&gt;

&lt;p&gt;The error for each data point is the distance from the line of best fit. This error is then squared; for our purposes it doesn’t matter if the error is above or below the line. The minimum mean squared error can then be represented by the red boxes below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://thepowerrank.com/wp-content/uploads/2018/08/regress_learn_4.png&quot; alt=&quot;Visual Representation of Minimum Minimum Squared Error from ThePowerRank.com&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How well does the red boxes explain the difference between the dependent variable and the independent variable?  The difference between the expected value and the mean squared error is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;&lt;em&gt;variance&lt;/em&gt;.&lt;/a&gt;  In simpler terms, variance measures how far the predicted values are away from the mean.  The proportion of the variance that is explained by the model (the distance from the actual data point to the linear best fit line) is the measure of how well the model replicates actual data.  This measure is called R-squared.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;R-squared is a statistic that will give some information about the goodness of fit of a model. In regression, the R-squared coefficient of determination is a statistical measure of how well the regression predictions approximate the real data points. An R-squared of 1 indicates that the regression predictions perfectly fit the data. &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;Coefficient of Determination&lt;/a&gt; on Wikipedia.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Referring to the red boxes mentioned before on ThePowerRank.com, the model represented by the best fit and the red boxes (recall they are a visual representation of the minimum mean squared error) account for 70% of the variance.  In other words, the model &lt;em&gt;explains&lt;/em&gt; 70% of the difference between the model and the actual data points.  Note the original variance is illustrated by the blue boxes below and that the volume of the red boxes are 70% less than the blue boxes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://thepowerrank.com/wp-content/uploads/2018/08/regress_learn_5.png&quot; alt=&quot;Visual Representation of the Variance and Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That still leaves 30% of the variance that the model does not explain.  The higher the R-squared value, the better the model illustrates and predicts real world data.&lt;/p&gt;

&lt;h3 id=&quot;using-r-to-model-money-line-minimums&quot;&gt;Using R to Model Money Line Minimums&lt;/h3&gt;
&lt;p&gt;Remember that Money Line winners are handicapped. A heavily favored team may have a Money line of -400 which means you are &lt;em&gt;laying&lt;/em&gt; 4 to 1 odds or betting $400 to win $100 if the team wins. Conversely, if a big underdog is listed at -600, you are getting 6 to 1 odds or betting $100 to win $600.&lt;/p&gt;

&lt;p&gt;Many touts have systems that claim to predict the winner of a game with 70% accuracy. The system may be incredible if it hits on bets that have an implied probability of less than 70%.  It’s a terrible system if you only win bets that have an implied probability greater than 70%. The real question we need to answer is whether the system will still be profitable based on different money line odds? And what are the minimum or maximum money line odds to ensure a profitable system? While it is important to have a system that has a high winning percentage, it is equally important to know the Money Line odds required to ensure you have a profitable system. &lt;a href=&quot;https://wizardofodds.com/games/sports-betting/&quot;&gt;Sports Betting Basics.&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;modeling-nfl-money-line-implied-probability-minimums&quot;&gt;Modeling NFL Money Line Implied Probability Minimums&lt;/h4&gt;
&lt;p&gt;Is a system with a 40% winning percentage high enough to be profitable &lt;em&gt;if&lt;/em&gt; the Money Line odds on all of the games it predicts average out to +125?  Or is a system that picks only big favorites profitable if the average money line odds are -200?&lt;/p&gt;

&lt;p&gt;To answer these questions, I will initially look at actual  Money Line data for the National Football League (“NFL”) going back to the 2007-2008 season.  Then, I will model the implied probabilities for favorites and underdogs.&lt;/p&gt;

&lt;h4 id=&quot;locating-and-preparing-the-money-line-data&quot;&gt;Locating and Preparing the Money Line Data&lt;/h4&gt;
&lt;p&gt;Part of the challenge of any data science project is &lt;em&gt;getting the data&lt;/em&gt;.  Luckily, with enough poking around the internet, you can find just about anything.  I downloaded Excel files with historical odds data from the &lt;a href=&quot;https://www.sportsbookreviewsonline.com/scoresoddsarchives/nfl/nfloddsarchives.htm&quot;&gt;NFL Scores and Odds Archive&lt;/a&gt;.  These were then read into R and analyzed.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;temp = list.files(pattern = &quot;^nfl&quot;)

NFL.Odds.Data &amp;lt;-
    lapply(temp,function(x)(
        read_excel(x,sheet = &quot;Sheet1&quot;)
    ))

NFL.Odds.Data &amp;lt;-
  rbindlist(NFL.Odds.Data)

NFL.Odds.Data$ML &amp;lt;- as.numeric(NFL.Odds.Data$ML)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This left me with data for approximately 9,100 teams that had money line info.  I then divided the money lines between positive (underdog team) and negative (favored team).  I also limited the upper and lower limits to only get Money Line information between -500 and 500.&lt;/p&gt;

&lt;h4 id=&quot;calculating-average-implied-probability-and-expected-value-for-nfl-favorites-and-underdogs&quot;&gt;Calculating Average Implied Probability and Expected Value for NFL Favorites and Underdogs&lt;/h4&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NFL.Odds.Data.Positive &amp;lt;-
  NFL.Odds.Data %&amp;gt;% select(ML) %&amp;gt;%
    group_by(ML) %&amp;gt;%
    filter(!is.na(ML),ML &amp;gt;= 10,ML &amp;lt;= 500)

NFL.Odds.Data.Negative &amp;lt;-
  NFL.Odds.Data %&amp;gt;% select(ML) %&amp;gt;%
    group_by(ML) %&amp;gt;%
    filter(!is.na(ML),ML &amp;gt;= -500,ML &amp;lt;= -10)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Both data sets were charted on a histogram and summarized.  A probability density function was overlaid on the histogram.  The vertical dashed red line on the graph represents the average.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.NFL.Odds.Data.Positive.png&quot; alt=&quot;Positive Money Line - Favorites&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.NFL.Odds.Data.Negative.png&quot; alt=&quot;Negative Money Line - Underdogs&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;summary(NFL.Odds.Data$ML)
      Min.    1st Qu.     Median       Mean    3rd Qu.
-105000.00    -200.00    -110.00     -47.39     170.00
      Max.       NA's
  35000.00        770
&amp;gt; summary(NFL.Odds.Data.Positive$ML)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  100.0   130.0   166.0   196.3   240.0   500.0
&amp;gt; summary(NFL.Odds.Data.Negative$ML)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   -500    -260    -180    -212    -141    -101
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Unsurprisingly, favorites are more expensive when compared with underdogs. On average, you would have to bet $212 on an NFL favorite to win $100, while a $100 bet on an average underdog pays out $196.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Convert American to Decimal Odds&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Positive Odds (+) = (MoneyLine/100) + 1
Negative Odds (-) = (100/MoneyLine) + 1

Average Underdog MoneyLine Decimal Odds (+) = 196.3/100 + 1 = 2.963
Average Favorite MoneyLine Decimal Odds (-) = 100/212.0 + 1 = 1.471
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Convert Decimal Odds to Implied Probability&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Implied Probability = (1/ decimal odds) * 100

Average Underdog MoneyLine Implied Probability (+) = (1/2.963) * 100 = 33.74%
Average Favorite MoneyLine Implied Probability (-) = (1/1.471) * 100 = 67.98%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Quick summary for what the odds imply for both the favorite and the underdog:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The odds imply a favorite wins 67.98% of the time; when this happens, you win $100 and when you lose, you lose $212.&lt;/li&gt;
  &lt;li&gt;An underdog wins 33.7% of the time, and you win $196 when successful and lose $100 when you are not.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The implied probabilities can then be used to calculate the &lt;strong&gt;expected value&lt;/strong&gt; (“EV”) for both bets.  “Expected value is a predicted value of a variable, calculated as the sum of all possible values each multiplied by the probability of its occurrence.”  See &lt;a href=&quot;https://help.smarkets.com/hc/en-gb/articles/214554985-How-to-calculate-expected-value-in-betting&quot;&gt;How to calculate expected value in betting&lt;/a&gt;.  The &lt;a href=&quot;https://towardsdatascience.com/what-is-expected-value-4815bdbd84de&quot;&gt;Expected Value (EV)&lt;/a&gt; can be calculated as follows in R:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pos.Expected.Value &amp;lt;-
  (NFL.Odds.Data.Positive.Implied.Prob*mean(NFL.Odds.Data.Positive$ML)) -
  ((1 - NFL.Odds.Data.Positive.Implied.Prob)* 100)

Neg.Expected.Value &amp;lt;-
  NFL.Odds.Data.Negative.Implied.Prob*100 -
  (1 -NFL.Odds.Data.Negative.Implied.Prob)*abs(mean(NFL.Odds.Data.Negative$ML))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This results in an EV of close to $0 for both underdogs (positive (+) money line) and favorites (negative (-) money line).  This makes sense because the bookmaker’s goal is to have balanced action on both sides - equal money on favorites and underdogs.  Theoretically then, based only on the implied probabilities, betting on just the favorites or just the underdogs should not give you a profitable system.&lt;/p&gt;

&lt;p&gt;But do the actual results support the expected results?  Since the odds imply a certain winning percentage, does the historical data match?  Turns out, favorites win 13% less than the odds imply and underdogs win about 9% more than the odds imply.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Implied Probability&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Actual Results&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Diff&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Underdog&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;33.74%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;36.82%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;+3.08%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Favorites&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67.98%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;59.89%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-8.09%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The actual results support the conclusion that oddsmakers likely bias their lines against favorites because the betting public bet favorites a lot more than underdogs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In 2004, University of Chicago economist Steven Levitt identified the fact that point spreads aren’t set like typical market prices, by equating relative levels of supply and demand. Instead, bookmakers set the margin to make the chance of the favorite covering the spread to be roughly 50 percent. Levitt speculated that bookmakers substantially improve their profits by biasing the spread very slightly against the favorite. This approach is profitable for bookmakers in part because, despite facing virtually even odds, people are much more likely to bet on the favorite than the underdog. See &lt;a href=&quot;https://fivethirtyeight.com/features/why-people-bet-on-the-favorite-even-when-the-spread-favors-the-underdog/&quot;&gt;Why People Bet on the Favorite Even When the Spread Favors the Underdog&lt;/a&gt; on FiveThirtyEight.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The actual results show that betting $100 on all the underdogs would be a profitable approach:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.PosML.Dogs.PL.png&quot; alt=&quot;Cumulative Profit and Loss for NFL Underdogs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The actual results for favorites is not so great - betting $100 game results in a substantial loss!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.NegML.Fav.PL.png&quot; alt=&quot;Cumulative Profit and Loss for NFL Underdogs&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;solving-for-the-required-system-winning-percentage-based-on-the-implied-money-line-odds&quot;&gt;Solving for the Required System Winning Percentage based on the Implied Money Line Odds&lt;/h4&gt;
&lt;p&gt;Implied probabilities can then be modeled for all money lines.  Think of the implied probability as the minimum winning percentage for your betting system to be profitable.  For instance, if your system only bet on favorites at -200 to win, then you would be required to win more than 66.67% to be profitable.&lt;/p&gt;

&lt;p&gt;The charts below show the minimum system probability percentage that you would need to be profitable.  Anything below the black line highlighted in red is non-profitable.  Anything above the line highlighted in green is profitable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.PosML.Dogs.Min.png&quot; alt=&quot;Implied Odds based on Positive Money Line&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.NegML.Fav.Min.png&quot; alt=&quot;Implied Odds based on Negative Money Line&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;

&lt;h3 id=&quot;an-overview-of-the-1976-system&quot;&gt;An Overview of the 1976 System&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://www.researchgate.net/profile/Raymond_Stefani/publication/291588643_Football_and_basketball_predictions_using_least_squares/links/582648c808aecfd7b8be88a1/Football-and-basketball-predictions-using-least-squares.pdf?origin=publication_detail&quot;&gt;rating described in the 1976 paper&lt;/a&gt; (the “1976 System”) referenced in the title of this post relates previous outcomes to team ratings.  Ratings seek to analyze the results of the competition and provide an objective measure of a team’s capabilities.  &lt;a href=&quot;https://en.wikipedia.org/wiki/Sports_rating_system&quot;&gt;Sports Ratings System&lt;/a&gt; on Wikipedia.  In brief, the 1976 System’s team rating is defined as the average opponent rating plus the team’s average win margin.  More specifically, the ratings are found as the team’s average win margin plus the 1/(n(i)+1) times the sum of each opponents average win margin.  I will attempt to recreate this system using different packages within R.&lt;/p&gt;

&lt;p&gt;Before we create our system, let’s chart the margin of victory distribution for our historical NFL data (note: margin of victory can be positive or negative - the winner is positive and the loser is negative).  It look’s roughly normally distributed.  The black curve in the chart below is a plot of the normal distribution, the bars are the actual margin of victory and the red is the average (which is zero).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.Margin.Victory.png&quot; alt=&quot;Distribution of NFL Margin of Victory&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The main assumption that forms the basis of the least square rating system is that the difference in scores between the winning team and the losing team is directly proportional to the difference in the ratings of the two teams. Therefore, the least squares rating system attempts to express the margin of victory as a linear function of the strengths of the playing teams. &lt;a href=&quot;https://homepages.cwi.nl/~schaffne/projects/reports/JoostMarysiaSportRatings14.pdf&quot;&gt;Sports Ratings.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, margin of victory in &lt;em&gt;past&lt;/em&gt; periods should predict margin of victory (and therefore a win or a loss) in &lt;em&gt;future&lt;/em&gt; periods.&lt;/p&gt;

&lt;h3 id=&quot;the-compres-and-comperank-packages&quot;&gt;The compres and comperank Packages&lt;/h3&gt;
&lt;p&gt;To help organize and sort the game data, I will use two R packages:  (1) &lt;a href=&quot;https://cran.r-project.org/package=comperes&quot;&gt;comperes&lt;/a&gt; which is an R tool to store and manage competition results, and (2) the companion package to compres, &lt;a href=&quot;https://cran.r-project.org/package=comperank&quot;&gt;comperank&lt;/a&gt; which ranks and rates the data provided.&lt;/p&gt;

&lt;h3 id=&quot;data-preparation-in-sample-and-out-of-sample-division&quot;&gt;Data Preparation: In-Sample and Out-Of Sample Division&lt;/h3&gt;
&lt;p&gt;First, divide the data into an in-sample and out-of-sample portions with an 80%/20% ratio of in-sample to out-of-sample. Next, use the compres package to create a tibble of head-to-head matchups with the mean score difference, cumulative score difference and number of wins columns.  Then, remove all of the head-to-head matchups between a team (and itself).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;q &amp;lt;- NFL.Odds.Data %&amp;gt;% select(Rot,Team,Final)
names(q) &amp;lt;- (c(&quot;game&quot;,&quot;player&quot;,&quot;score&quot;))
q1 &amp;lt;- rep(1:(length(q$game)/2),each = 2)
q$game &amp;lt;- q1
q &amp;lt;- q %&amp;gt;% filter(!is.na(score))
q.in.sample &amp;lt;- q %&amp;gt;% subset(game &amp;lt;= length(q$game)/2*.8)
q.out.sample &amp;lt;- q %&amp;gt;% subset(game &amp;gt; length(q$game)/2*.2)
q3 &amp;lt;- q.in.sample %&amp;gt;% h2h_long(!!! h2h_funs)
q3 &amp;lt;- q3 %&amp;gt;% filter(!is.na(num_wins),!is.na(mean_score_diff),player1 != player2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;simple-linear-regression&quot;&gt;Simple Linear Regression&lt;/h3&gt;
&lt;p&gt;The correlation matrix for the data is as follows:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;               mean_score_diff mean_score_diff_pos mean_score sum_score_diff sum_score_diff_pos sum_score num_wins num_wins2   num
mean_score_diff                1.00                0.84       0.74           0.66               0.53      0.13     0.26      0.26  0.00
mean_score_diff_pos            0.84                1.00       0.64           0.55               0.56     -0.01     0.11      0.11 -0.12
mean_score                     0.74                0.64       1.00           0.49               0.38      0.13     0.16      0.16 -0.04
sum_score_diff                 0.66                0.55       0.49           1.00               0.79      0.19     0.40      0.40  0.00
sum_score_diff_pos             0.53                0.56       0.38           0.79               1.00      0.52     0.65      0.65  0.38
sum_score                      0.13               -0.01       0.13           0.19               0.52      1.00     0.94      0.94  0.97
num_wins                       0.26                0.11       0.16           0.40               0.65      0.94     1.00      1.00  0.89
num_wins2                      0.26                0.11       0.16           0.40               0.65      0.94     1.00      1.00  0.89
num                            0.00               -0.12      -0.04           0.00               0.38      0.97     0.89      0.89  1.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The average score difference for team 1 vs team 2 (referred to as “player 1” and “player 2” in the compres package) and the mean score of team 1 vs team 2 is positively correlated by 0.74.  For simple linear regression, the format for the lm() function is “YVAR ~ XVAR” where YVAR is the dependent, or predicted, variable and XVAR is the independent, or predictor, variable.  &lt;a href=&quot;https://www.r-bloggers.com/r-tutorial-series-simple-linear-regression/&quot;&gt;R Tutorial Series: Simple Linear Regression&lt;/a&gt;.  The hypothesis is that the average points scored by a given team in the past can predict the margin of victory in future matchups.  Our dependent or predicted y-variable then is “mean_score_diff” and our x-variable (the predictor variable) is “mean_score”.&lt;/p&gt;

&lt;p&gt;A quick scatter plot of the in-sample data for “mean_score_diff” vs the “mean_score” shows a linear relationship between the two.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/CH.q5.png&quot; alt=&quot;Plot of Number of Wins vs Positive Score Difference&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A summary of the linear regression:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Call:
lm(formula = q3$mean_score_diff ~ q3$mean_score)

Residuals:
    Min      1Q  Median      3Q     Max
-33.726  -3.755   0.236   3.734  20.483

Coefficients:
               Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)   -24.54641    0.69118  -35.51   &amp;lt;2e-16 ***
q3$mean_score   1.08181    0.02931   36.91   &amp;lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.397 on 1156 degrees of freedom
Multiple R-squared:  0.5409,	Adjusted R-squared:  0.5405
F-statistic:  1362 on 1 and 1156 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The $R^2$ value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, 54% of the variability in score differential is explained by the historical average score of team 1 vs team 2.&lt;/p&gt;

&lt;p&gt;Based on the regression, the difference in team 1 score vs team 2 score is equal to:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mean_score_diff = -24.56 + 1.08181 * mean_score
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Using the predict() function, the mean squared error (“MSE”) for the in-sample data was 6.39 compared with an MSE of 7.53 for the out-of-sample data.&lt;/p&gt;

&lt;p&gt;But recall - we are not concerned with &lt;em&gt;win or loss margin&lt;/em&gt; even though the model we just created attempts to predict it. Rather, we are looking for the predicted &lt;em&gt;winner&lt;/em&gt; of the game so that we can bet the Money Line.  How often does the model predict the positive/negative margin of victory then? For the in-sample data, the model predicted the correct positive/negative margin of victory correctly 78.50% of the time.  The out-of-sample data did not do as well - the model only predicted it 70.80% of the time!&lt;/p&gt;

&lt;p&gt;But we are not concerned with the accuracy of the margin of victory OR how often it was predicted correctly.  When you drill down to wins and losses, the model predicted the Money Line winner 61.89% in-sample and 61.29% out-of-sample.  According to the original paper, they hit on winners in the NFL of 67.7%.  Based on the large difference between the two, our model needs a little work!&lt;/p&gt;

&lt;h3 id=&quot;multiple-regression&quot;&gt;Multiple Regression&lt;/h3&gt;
&lt;p&gt;One option to improve the model is to use multiple linear regression on the data.  In essence, we are adding more data to help project the outcome.  Our multiple linear regression model is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fit &amp;lt;- lm(q3$mean_score_diff~q3$mean_score_diff_pos+q3$mean_score+q3$sum_score_diff+q3$sum_score_diff_pos+q3$sum_score+q3$num_wins)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Does the extra data lead to better accuracy when predicting postive or negative margin of victory?  The $R^2$ value increased to 0.85, and the model predicted whether the whether the margin of victory would be positive or negative approximately 91% of the time for in-sample data, and 85.49% for the out-of-sample data.&lt;/p&gt;

&lt;p&gt;The multiple regression also increased the accuracy for predicting Money Line winners (using the positive or negative approach).  In-sample accuracy increased to 65.81% and (curiously) out-of-sample data accuracy improved to 70.33%.&lt;/p&gt;

&lt;h2 id=&quot;sources-and-notes&quot;&gt;Sources and Notes&lt;/h2&gt;
&lt;h3 id=&quot;overview-of-predictive-systems-for-sports&quot;&gt;Overview of Predictive Systems for Sports&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.investopedia.com/terms/l/least-squares-method.asp&quot;&gt;Least Squares Method Definition on Investopedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Raymond_Stefani/publication/291588643_Football_and_basketball_predictions_using_least_squares/links/582648c808aecfd7b8be88a1/Football-and-basketball-predictions-using-least-squares.pdf?origin=publication_detail&quot;&gt;Football and Basketball Predictions Using Least Squares&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://homepages.cae.wisc.edu/~dwilson/rsfc/rate/papers/bcs.pdf&quot;&gt;LEAST SQUARES MODEL FOR PREDICTING COLLEGE FOOTBALL SCORES&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/fe4f/123780e0eaf8418243e14d791d19c815ee50.pdf&quot;&gt;COLLEGE FOOTBALL: A MODIFIED LEAST SQUARES APPROACH TO RATING AND PREDICTION&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.1843magazine.com/features/the-daily/how-i-used-maths-to-beat-the-bookies&quot;&gt;How I Used Maths to Beat the Bookies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.stats.com/wp-content/uploads/2018/09/2011.pdf&quot;&gt;The Problem with Win Probability&lt;/a&gt; - Paper from the MIT Sloan Sports Analytics Conference.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.masseyratings.com/theory/massey97.pdf&quot;&gt;Statistical Models Applied to the Rating of Sports Teams&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www3.nd.edu/~apilking/Math10170/Information/Lectures%202015/Topic%209%20Massey's%20Method.pdf&quot;&gt;Massey’s Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://omfgpwn3d.livejournal.com/5069.html&quot;&gt;Massey’s “Game Outcome Function”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.thepredictiontracker.com/football.php&quot;&gt;Prediction Tracker Archive - Football&lt;/a&gt; on the Prediction Tracker.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://homepages.cae.wisc.edu/~dwilson/rsfc/rate/biblio.html&quot;&gt;Bibliography on College Football Ranking Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;odds-implied-probability-and-margin&quot;&gt;Odds, Implied Probability and Margin&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://help.smarkets.com/hc/en-gb/articles/214058369-How-to-calculate-implied-probability-in-betting&quot;&gt;How to calculate implied probability in betting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://help.smarkets.com/hc/en-gb/articles/214180145-How-to-calculate-betting-margins&quot;&gt;How to Calculate Betting Margin&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://help.smarkets.com/hc/en-gb/sections/360003674031-Trading-and-Betting-Basics&quot;&gt;Trading and Betting Basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Information on converting Decimal, Fractional and American Odds along with a odds calculator can be found here:  (1)  &lt;a href=&quot;http://www.betsmart.co/odds-conversion-formulas/#americantodecimal&quot;&gt;Odds Conversion Formulas&lt;/a&gt; on BetSmart.co; and (2) &lt;a href=&quot;https://help.smarkets.com/hc/en-gb/articles/214062929-How-to-convert-betting-odds&quot;&gt;How to Convert Betting Odds&lt;/a&gt; on Smarkets.com.&lt;/li&gt;
  &lt;li&gt;For implied probability calculations, check out &lt;a href=&quot;https://help.smarkets.com/hc/en-gb/articles/214058369-How-to-calculate-implied-probability-in-betting&quot;&gt;How to calculate implied probability in betting&lt;/a&gt;.  You can use the implied probabilities to determine &lt;a href=&quot;https://help.smarkets.com/hc/en-gb/articles/214554985-How-to-calculate-expected-value-in-betting&quot;&gt;How to calculate expected value in betting&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/mens-machine-learning-competition-2018/discussion/52253&quot;&gt;Historical Vegas Odds Discussion&lt;/a&gt; on Kaggle.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sportsbookreviewsonline.com/scoresoddsarchives/nfl/nfloddsarchives.htm&quot;&gt;NFL Scores and Odds Archive&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;least-squares-and-regression&quot;&gt;Least Squares and Regression&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mathsisfun.com/data/least-squares-regression.html&quot;&gt;Least Squares Regression&lt;/a&gt; on Math is Fun.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.investopedia.com/terms/l/least-squares-method.asp&quot;&gt;Least Squares Method Definition&lt;/a&gt; on Investopedia.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://r-statistics.co/Linear-Regression.html&quot;&gt;Linear Regression&lt;/a&gt; Overview by r-statistics.co&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.automatingthefuture.com/blog/2017/5/9/making-predictions-with-simple-linear-regression-models&quot;&gt;Making Predictions With Simple Linear Regression Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rafalab.github.io/dsbook/linear-models.html#lse&quot;&gt;Introduction to Data Science Book - 19.3 Least Squared Estimates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b&quot;&gt;The Linear Algebra View of Least-Squares Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.learnbymarketing.com/tutorials/linear-regression-in-r/&quot;&gt;Linear Regression Example in R using lm() Function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.r-bloggers.com/ordinary-least-squares-ols-linear-regression-in-r/&quot;&gt;Ordinary Least Squares (OLS) Linear Regression in R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://people.duke.edu/~rnau/regexbaseball.htm&quot;&gt;A simple regression example: predicting baseball batting averages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://datascienceplus.com/linear-regression-from-scratch-in-r/&quot;&gt;Linear Regression from Scratch in R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.rpubs.com/ajbayquen/176851&quot;&gt;Introduction to linear regression&lt;/a&gt; on RPubs.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.r-bloggers.com/r-tutorial-series-simple-linear-regression/&quot;&gt;R Tutorial Series: Simple Linear Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://campus.datacamp.com/courses/machine-learning-toolbox/regression-models-fitting-them-and-evaluating-their-performance?ex=8&quot;&gt;Data Camp Tutorial on Predict() Function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/&quot;&gt;Writing Mathematic Fomulars in Markdown&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.statmethods.net/stats/regression.html&quot;&gt;Multiple (Linear) Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-comperes-comprank-and-playerratings-packages&quot;&gt;The comperes, comprank and PlayerRatings Packages&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.r-bloggers.com/harry-potter-and-competition-results-with-comperes/&quot;&gt;Harry Potter and competition results with comperes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/comperank/vignettes/methods-overview.html&quot;&gt;comperes Vignette&lt;/a&gt; and &lt;a href=&quot;https://cran.r-project.org/web/packages/comperes/comperes.pdf&quot;&gt;comperes Manual&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/PlayerRatings/vignettes/AFLRatings.pdf&quot;&gt;PlayerRatings Vignette&lt;/a&gt; and &lt;a href=&quot;https://cran.r-project.org/web/packages/PlayerRatings/PlayerRatings.pdf&quot;&gt;PlayerRatings Manual&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jake Stoetzner</name><email>Jake.stoetzner@gmail.com</email></author><category term="r" /><category term="sports-betting" /><summary type="html">Creating a sports betting system that beats the bookmaker is, to say the least, a challenge. In this post, I create a betting system using least squares regression that has an accuracy rate of up to 70% using out-of-sample data.</summary></entry><entry><title type="html">Monte Carlo Benchmarking for Trading Systems in R</title><link href="http://localhost:4000/r/backtesting/2019/10/01/Monte-Carlo-Benchmarking-for-Trading-Systems-in-R/" rel="alternate" type="text/html" title="Monte Carlo Benchmarking for Trading Systems in R" /><published>2019-10-01T00:00:00-05:00</published><updated>2019-10-01T00:00:00-05:00</updated><id>http://localhost:4000/r/backtesting/2019/10/01/Monte-Carlo-Benchmarking-for-Trading-Systems-in-R</id><content type="html" xml:base="http://localhost:4000/r/backtesting/2019/10/01/Monte-Carlo-Benchmarking-for-Trading-Systems-in-R/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Part of the challenge of backtesting is coming up with a repeatable framework to compare trading systems.  In this example, I will use Monte Carlo simulation in R as a way to measure whether or not a particular trading system is better than pure chance.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h4 id=&quot;overview&quot;&gt;Overview&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#research&quot;&gt;Research&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#monte-carlo-methods&quot;&gt;Monte-Carlo Methods&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#coin-flip-systems-and-random-returns&quot;&gt;Coin Flip Systems and Random Returns&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#monte-carlo-simulations-of-the-coin-flip-system&quot;&gt;Monte Carlo Simulations of the Coin Flip System&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#a--slightly--more-complicated-trading-system--bollinger-band-strategy&quot;&gt;A (Slightly) More Complicated Trading System: Bollinger Band Strategy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#monte-carlo-simulation-of-the-bollinger-band-strategy&quot;&gt;Monte Carlo Simulation of the Bollinger Band Strategy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusions&quot;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#research-and-notes&quot;&gt;Research and Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;research&quot;&gt;Research&lt;/h2&gt;

&lt;h4 id=&quot;monte-carlo-methods&quot;&gt;Monte-Carlo Methods&lt;/h4&gt;
&lt;p&gt;“Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results; typically one runs simulations many times over in order to obtain the distribution of an unknown probabilistic entity. The name comes from the resemblance of the technique to the act of playing and recording results in a real gambling casino. “ [Wikipedia.(http://en.wikipedia.org/wiki/Monte_Carlo_methods)&lt;/p&gt;

&lt;p&gt;A trading system is nothing more than a collection of rules to enter a long (+1), short (-1) or neutral position (0) in a financial instrument.  “If the rule that determines the position to take each time a trading opportunity arises is an intelligent rule, the sum of the obtained returns will be larger than the sum that could be expected from a rule that assigns positions randomly.” &lt;a href=&quot;https://www.evidencebasedta.com/montedoc12.15.06.pdf&quot;&gt;See Monte-Carlo Evaluation of Trading Systems&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At its core, Monte-Carlo simulation is a method that: (1) models a problem, (2) randomly simulates the outcome a large number of times, and (3) applies statistical analysis to the output to evaluate the model’s probabilistic characteristics. You can then compare the Monte-Carlo simulation to the trading system results to determine if they were obtained by luck or through a &lt;em&gt;real&lt;/em&gt; trading edge.&lt;/p&gt;

&lt;p&gt;Whether a trading system has an edge or has attained its returns through randomness can be tested (relatively) easy in R.  In statistical terms, a Monte-Carlo simulation tests the null hypothesis that the pairing of long/short/neutral positions with raw returns is random.  &lt;a href=&quot;https://www.evidencebasedta.com/montedoc12.15.06.pdf&quot;&gt;See Monte-Carlo Evaluation of Trading Systems&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://stattrek.com/statistics/dictionary.aspx?definition=null_hypothesis&quot;&gt;Null Hypothesis&lt;/a&gt; is the hypothesis that sample observations result purely from chance.  The &lt;a href=&quot;https://stattrek.com/statistics/dictionary.aspx?definition=null_hypothesis&quot;&gt;Alternative Hypothesis&lt;/a&gt; “is the hypothesis that sample observations are influenced by some non-random cause.””&lt;/p&gt;

&lt;p&gt;In trading terms, we are trying to determine whether the system is intelligently designed to produce superior returns (our &lt;em&gt;Alternative Hypothesis&lt;/em&gt;) as compared with a random benchmark (our &lt;em&gt;Null Hypothesis&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;“A significance test is performed by assuming the so-called null hypothesis, which asserts that the measured effect occurs due to sampling error alone. If the null hypothesis is rejected, it’s concluded that the measured effect is due to something more than just sampling error (i.e., it’s significant). To determine whether the null hypothesis should be rejected, a significance or confidence level is chosen. For example, a significance level of 0.05 represents a confidence level of 95%. The so-called p-value is the probability of obtaining the measured statistic if the null hypothesis is true. The smaller the p-value the better. If the p-value is less than the significance level (e.g., p &amp;lt; 0.05), then the null hypothesis is rejected, and the test statistic is deemed to be statistically significant.”  See&lt;a href=&quot;http://www.adaptrade.com/Newsletter/NL-GoodOrLucky.htm&quot;&gt; Is That Back-Test Result Good or Just Lucky?&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;coin-flip-systems-and-random-returns&quot;&gt;Coin Flip Systems and Random Returns&lt;/h4&gt;
&lt;p&gt;A completely random trading system enters and exits the market without regard to the past, current or the future or any outside indicators.&lt;/p&gt;

&lt;p&gt;Assume we create a trading system that enters and exits an ETF at the end of each trading day based on the outcome of a flip of a coin.  We assume a fair coin and that a heads (a “1”) means we are long the market and a tails (a “-1”) means we are short the market.  We repeat this daily - going long or going short each day.  See &lt;a href=&quot;http://mindrighttrading.blogspot.com/2013/02/coin-flips-risk-to-reward-profile-and.html&quot;&gt;My Old Blog For More Information on This.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We have two input time series for the trading system:  (1) the &lt;strong&gt;Rule Position&lt;/strong&gt; which is the coin flip entry rule be long or short; and (2) the &lt;strong&gt;Market&lt;/strong&gt; or &lt;strong&gt;Raw Return&lt;/strong&gt; which is the daily return of the financial instrument.  See&lt;a href=&quot;http://www.automated-trading-system.com/monte-carlo-permutation/&quot;&gt;Monte Carlo Permutation: Test your Back-Tests&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our &lt;em&gt;Null Hypothesis&lt;/em&gt; is that half of the flips will be heads and half of the flips will be tails, i.e., we will be long half the time and short half the time.  Stated otherwise, the returns of the Coin Flip System are a randomly correlated to Market or Raw returns.  The &lt;em&gt;Alternative Hypothesis&lt;/em&gt; is that the Coin Flip System is a sample from a profitable population, and that the Rule Position is correlated to market returns.&lt;/p&gt;

&lt;p&gt;Below is the code to simulate coin flips with a 50% probability of coming up with either heads or tails.  See &lt;a href=&quot;http://rstudio-pubs-static.s3.amazonaws.com/8492_b817c712a5f6456fb4c5932e3d957135.html#/1&quot;&gt;Simulating a Coin Flip in R.&lt;/a&gt; Note that we are flipping the coin a number of times equal to the number of trading days for QQQ.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sample.space &amp;lt;- c(-1,1)
theta &amp;lt;- 0.5  ##this is a fair coin
N &amp;lt;- length(Close[[1]]) ## match number of flips to total trading days

flips &amp;lt;- sample(sample.space,
                size = N,
                replace = TRUE,
                prob = c(theta, 1 - theta))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we apply the “coin flip” outcome (which is our Rule Position) to the daily returns of QQQ (which is our Raw or Market Return) and then plot the cumulative returns during the trading period.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Coin.Flip.System &amp;lt;-
  all.stock.data[[1]] %&amp;gt;%
    mutate(
    daily.return = ifelse(row_number() == 1, 0, close / lag(close, 1) - 1),

    signal.return = daily.return *  flips,

    cum.return = cumprod(1+signal.return) - 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using our coin-flip trading system, let’s apply it to the daily returns of the Nasdaq-100 based ETF &lt;a href=&quot;https://finance.yahoo.com/quote/QQQ/&quot;&gt;QQQ&lt;/a&gt;.  The chart below shows the cumulative daily returns of the Coin Flip System.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Coin.Flip.1.png&quot; alt=&quot;Coin Flip System - One Iteration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is only one iteration/run of our trading system.  What if we want to know &lt;strong&gt;all&lt;/strong&gt; of the possible paths the ETF could take?&lt;/p&gt;

&lt;h4 id=&quot;monte-carlo-simulations-of-the-coin-flip-system&quot;&gt;Monte Carlo Simulations of the Coin Flip System&lt;/h4&gt;

&lt;p&gt;Solve for the standard deviation and average return of the Coin Flip System, and then find the average daily return and the average standard deviation of these daily returns of the system.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MC.Coin.Flip.System &amp;lt;-
  all.stock.data[[1]] %&amp;gt;%
    mutate(
    daily.return = ifelse(row_number() == 1, 0, close / lag(close, 1) - 1),

    signal.return = daily.return *  flips,

    cum.return = cumprod(1+signal.return) - 1,

    sd = runSD(daily.return, n = 252)*sqrt(252),

    sd.2 = runSD(daily.return))

MC.Coin.Flip.System.sd &amp;lt;- mean(MC.Coin.Flip.System$sd.2, na.rm = T)

MC.Coin.Flip.System.avg &amp;lt;- mean(MC.Coin.Flip.System$daily.return, na.rm = T)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we create a function that projects a &lt;a href=&quot;https://en.wikipedia.org/wiki/Normal_distribution&quot;&gt;normal distribution&lt;/a&gt; of the daily returns of the Coin Flip System.  We apply those different paths to the closing value of QQQ from the first date we evaluate the ETF (here it is 2016-01-03) to the final closing price.  Much of this code is taken from the post &lt;a href=&quot;https://www.countbayesie.com/blog/2015/3/3/6-amazing-trick-with-monte-carlo-simulations&quot;&gt;Monte Carlo Simulations in R&lt;/a&gt; on the incredible &lt;a href=&quot;https://www.countbayesie.com/&quot;&gt;Count Bayesie Blog.&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MC.Coin.Flip.System.generate.path &amp;lt;- function(){

  MC.Coin.Flip.System.days &amp;lt;- length(Close[[1]])

  MC.Coin.Flip.System.changes &amp;lt;-

  rnorm(length(Close[[1]]),mean=(1+MC.Coin.Flip.System.avg),sd=MC.Coin.Flip.System.sd)

  MC.Coin.Flip.System.sample.path &amp;lt;- cumprod(c(Open[[1]][[1]],MC.Coin.Flip.System.changes))

  MC.Coin.Flip.System.closing.price &amp;lt;- MC.Coin.Flip.System.sample.path[days+1]

  return(MC.Coin.Flip.System.closing.price/Open[[1]][[1]]-1)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now replicate the Coin Flip System path 100,000 times and then solve for the median value of the stock and the upper and lower 95th percentiles.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MC.Coin.Flip.System.runs &amp;lt;- 100000
MC.Coin.Flip.System.closing &amp;lt;- replicate(runs,MC.Coin.Flip.System.generate.path())

median(MC.Coin.Flip.System.closing)
quantile(MC.Coin.Flip.System.closing,probs = c(0.05,0.95))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Repeat the Monte Carlo Simulation process for the underlying ETF by solving for the average daily return and standard deviation then simulate 100,000 different paths QQQ &lt;em&gt;could&lt;/em&gt; have taken.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Coin Flip System Return (%)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ETF Return (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Median&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;385%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;386%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Mean&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;498%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;499%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;95th Percentile&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1307%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1310%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5th Percentile&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;66.3%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;66.5%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The similarities between the Coin Flip System and the market returns for the ETF are virtually the same.  Why?  Mainly because of the similarities between the average and standard deviation of the Coin Flip System and the ETF itself.  Sine the Coin Flip System is either long or short the underlying ETF, it was highly probable that the system and the ETF would have the same statistical characteristics.&lt;/p&gt;

&lt;p&gt;The chart below models &lt;strong&gt;one&lt;/strong&gt; of the simulated returns for our Coin Flip System (in red) and &lt;strong&gt;one&lt;/strong&gt; of the simulated returns for the underlying QQQ ETF (in blue).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Coin.Flip.2.png&quot; alt=&quot;Monte Carlo Simulation of Coin Flip System&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Comparing the median and confidence intervals of the Coin Flip System vs the random entry into the ETF, we cannot reject the Null Hypothesis that the Coin Flip System results were purely by chance.  From a practical standpoint, we knew this going in because the entry and exit &lt;em&gt;is purely random&lt;/em&gt; being entirely based on an equal (50/50 chance of either being long or short.&lt;/p&gt;

&lt;p&gt;But what if you can’t use the “eye test” for evaluating whether or not the results were random?&lt;/p&gt;

&lt;p&gt;“The p-value of the original back-testing sample can then be computed (it is equal to the fraction of random rule returns equal or greater to the back-tested rule return).”  &lt;a href=&quot;http://www.automated-trading-system.com/monte-carlo-permutation/&quot;&gt;Source.&lt;/a&gt;  In this example, the average ETF cumulative return is 499%.  The Null Hypothesis is that the cumulative return of the Coin Flip System in excess of the average ETF cumulative return are random.&lt;/p&gt;

&lt;p&gt;Calculate the t-test distribution and the p-value in excess of the average return.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;t &amp;lt;- (mean(MC.Coin.Flip.System.closing,na.rm = T)-mean(mc.closing))/(sd(MC.Coin.Flip.System.closing)/sqrt(length(MC.Coin.Flip.System.closing)))

p &amp;lt;- 2*pt(-abs(t),df=length(MC.Coin.Flip.System.closing)-1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This results in a p-value for the Coin Flip System of 0.71.  This is well short of the statistical significance threshhold of 0.05 for a p-value (the null hypothesis would be rejected for any p-value less than 0.05).  We cannot reject the null hypothesis that the Coin Flip System cumulative return is not by chance.&lt;/p&gt;

&lt;p&gt;Although it is likely self evident, you would not want to trade the Coin Flip System &lt;em&gt;even if&lt;/em&gt; you got a result showing that the system had a very positive return.  If you look at the chart above it appears that the Coin Flip System significantly outperforms being long the ETF.  This is a &lt;a href=&quot;https://www.stat.berkeley.edu/~aldous/157/Papers/harvey.pdf&quot;&gt;fluke&lt;/a&gt; - a false trading strategy that, in reality, was just one path the strategy could have taken.  I will discuss false trading strategies in a separate post.&lt;/p&gt;

&lt;h4 id=&quot;a-slightly-more-complicated-trading-system-bollinger-band-strategy&quot;&gt;A (Slightly) More Complicated Trading System: Bollinger Band Strategy&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://school.stockcharts.com/doku.php?id=technical_indicators:bollinger_bands&quot;&gt;Bollinger Bands&lt;/a&gt; measure the historical volatility of an instrument above and below a simple moving average.  A simple Bollinger Bands system is a mean reverting strategy that would enter long when price crossed over the lower band and enter short when price crosses the upper band.  This simple strategy is always exposed to the market; it is never flat.&lt;/p&gt;

&lt;p&gt;Although the &lt;a href=&quot;https://github.com/joshuaulrich/TTR&quot;&gt;TTR Package in R&lt;/a&gt; supports the use of Bollinger Bands, I will construct them from scratch.&lt;/p&gt;

&lt;p&gt;First, assign the length for the simple moving average (the middle band) and the average for the upper and lower bands.  Here, I use the traditional SMA setting of 20 periods.&lt;/p&gt;

&lt;p&gt;Assign the displacement for the upper and lower bands. I set the upper and lower bands 2 standard deviations above and below the simple moving average.&lt;/p&gt;

&lt;p&gt;Using the OHLC from the ETF, set the rolling standard deviations and calculate the middle, upper and lower bands.  Our signal will enter long (+1) if the close is less than lower band and enter short (-1) if the close is greater than the upper band. Note that denoting a short as “-1” and then multiplying that by the daily return of the instrument, gives us the correct return.  For example, if the ETF is -1% for the day amd we are short, the end result is a gain of 1%.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BBand.Avg.Length &amp;lt;- 20

BBand.SD &amp;lt;- 2

BBand.System &amp;lt;-
  all.stock.data[[1]] %&amp;gt;%

    select(date,open,high,low,close) %&amp;gt;%

    mutate(
      daily.return = ifelse(row_number() == 1, 0, close / lag(close, 1) - 1),

      cum.daily.return =
        (cumprod(1+daily.return)-1)*100,

      sd = runSD(close),

      middle.band = ifelse(row_number() &amp;gt;= BBand.Avg.Length,  rollmean(close, k = BBand.Avg.Length, fill = NA), 0),

      upper.band = ifelse(row_number() &amp;gt;= BBand.Avg.Length,  rollmean(close, k = BBand.Avg.Length, fill = NA) + (BBand.SD * rollmean(sd, k = BBand.Avg.Length, fill = NA, 0)),0),

      lower.band = ifelse(row_number() &amp;gt;= BBand.Avg.Length,  rollmean(close, k = BBand.Avg.Length, fill = NA) - (BBand.SD * rollmean(sd, k = BBand.Avg.Length, fill = NA, 0)),0),

      signal =
        ifelse(close &amp;lt; lower.band,1,
          ifelse(close &amp;gt; upper.band,-1,NA)),

      signal.2 =
        na.locf(signal),

      signal.return = daily.return *  signal.2,

      cum.signal.return = (cumprod(1+signal.return) - 1)*100)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The chart below shows the comparison of the cumulative daily returns of QQQ (in red) vs the Bollinger Bands strategy (in blue).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/BBand.Ch.1.png&quot; alt=&quot;QQQ Returns vs Bollinger Bands Strategy Returns&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;monte-carlo-simulation-of-the-bollinger-band-strategy&quot;&gt;Monte Carlo Simulation of the Bollinger Band Strategy&lt;/h4&gt;
&lt;p&gt;Analyzing the Bollinger Band Strategy as compared to being long the QQQ ETF, and assuming a 13.5 year holding period from 2006-01-01 to 2019-06-30, the results are as follows:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;BBands Strategy&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;QQQ ETF&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average Daily Return&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.08%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.05%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average Standard Deviation&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.13%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.47%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Maximum Drawdown&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-444.79%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-324.67%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average Sharpe Ratio&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.73&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Cumulative Return&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1114.30%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;352.04%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Annualized Return&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20.30%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11.82%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Using the same Monte Carlo Analysis above on the Bollinger Band Strategy and utilizing 100,000 runs, the median cumulative return for the Bollinger Bands strategy is 1195%, with a 95% confidence return between 340% to 3722%.&lt;/p&gt;

&lt;p&gt;For the ETF, Monte Carlo Analysis returns a median cumulative return of 382%, average cumulative return of 499%, with a 95% confidence return between 62% to 1325%.&lt;/p&gt;

&lt;p&gt;Solving for the t-distribution of the excess returns of the Bollinger Band Strategy versus the  ETF yields 267.90.  This yields an extremely small p-value; it is essentially zero, which means we can reject the null hypothesis.  In other words, it is likely that the Bollinger Band strategy provides an edge.  This is not a recommendation to trade this strategy.  More testing is needed to determine if this strategy is viable.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Monte Carlo methods are a good tool to have to determine if a given trading strategy is due to chance or (rather) from a statistically significant edge.  In future posts, I will examine other methods to determine if the strategy is tradeable.  These include &lt;a href=&quot;https://www.uvm.edu/~dhowell/StatPages/ResamplingWithR/BootstrappingR.html&quot;&gt;Bootstrapping&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Jackknife_resampling&quot;&gt;Jacknife&lt;/a&gt;, &lt;a href=&quot;http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-20279-2&quot;&gt;Permutation Testing&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot;&gt;Cross-Validation.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-and-notes&quot;&gt;Research and Notes&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;For an excellent read on how to construct your own “synthetic” security, check out &lt;a href=&quot;http://www.tvmcalcs.com/blog/comments/coin_tosses_and_stock_price_charts/&quot;&gt;Coin Tosses and Stock Price Charts&lt;/a&gt; by Timothy R. Mayes, Ph.D.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A  Brief Overview of Bollinger Bands.  &lt;a href=&quot;https://school.stockcharts.com/doku.php?id=technical_indicators:bollinger_bands&quot;&gt;Stock Charts - Bollinger Bands.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An Overview of Using Moving Averages in R. &lt;a href=&quot;http://uc-r.github.io/ts_moving_averages#centered-moving-averages&quot;&gt;UC Business Analytics R Programming Guide - Moving Averages.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pitfalls of Using Monte Carlo Analysis.  &lt;a href=&quot;http://systemtradersuccess.com/fooled-monte-carlo-analysis/&quot;&gt;System Trader Success - Fooled by Monte Carlo Analysis.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Trading System Evaluation.  &lt;a href=&quot;https://www.mesasoftware.com/papers/SystemEvaluation.pdf&quot;&gt;Evaluating Trading Systems By John Ehlers and Ric Way.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An incredible overview of Monte Carlo Analysis Permutation Usage when evaluating trading systems. &lt;a href=&quot;https://www.evidencebasedta.com/MonteDoc12.15.06.pdf&quot;&gt;Evidence Based Technical Analysis - Monte Carlo Permutation Evaluation of Trading Systems.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Table of contents generator for &lt;a href=&quot;https://ecotrust-canada.github.io/markdown-toc/&quot;&gt;MarkDown.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://mindrighttrading.blogspot.com/2013/02/coin-flips-risk-to-reward-profile-and.html&quot;&gt;Coin Flips, Risk to Reward Profile and Creating Your Own Synthetic Security&lt;/a&gt; from my old blog Mind Right Trading.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An interesting paper on &lt;a href=&quot;https://www.stat.berkeley.edu/~aldous/157/Papers/harvey.pdf&quot;&gt;Evaluating Trading Strategies&lt;/a&gt;. The authors review several methods for calculating a t-statistic formula using the Sharpe Ratio of the trading strategy. This is calculated by multiplying the Sharpe Ratio times the square root of the number of years the strategy traded.  For instance, a t-statistic of 2.91 means that the observed profitability is three standard deviations from the null hypothesis of zero profitability.  The paper expands on this formula, covering the problems of multi-testing trading strategies by using the family-wise error rate (FWER) and the false discovery rate (FDR).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2 great articles on the specifics of Monte Carlo Permutation when evaluating trading strategies:  (1) &lt;a href=&quot;http://www.automated-trading-system.com/monte-carlo-permutation/&quot;&gt;Monte Carlo Permutation: Test your Back-Tests&lt;/a&gt;; and (2) &lt;a href=&quot;http://www.adaptrade.com/Newsletter/NL-GoodOrLucky.htm&quot;&gt;Is That Back-Test Result Good or Just Lucky?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An intense and lengthy overview on StackExchange of &lt;a href=&quot;https://stats.stackexchange.com/questions/104040/resampling-simulation-methods-monte-carlo-bootstrapping-jackknifing-cross&quot;&gt;Resampling / simulation methods: monte carlo, bootstrapping, jackknifing, cross-validation, randomization tests, and permutation tests.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cyclismo.org/tutorial/R/pValues.html#the-easy-way&quot;&gt;Chapter 10&lt;/a&gt; of the very helpful &lt;a href=&quot;http://www.cyclismo.org/tutorial/R/&quot;&gt;R Tutorial&lt;/a&gt; shows several ways of calculating a p-value.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jake Stoetzner</name><email>Jake.stoetzner@gmail.com</email></author><category term="r" /><category term="backtesting" /><category term="Monte-Carlo" /><summary type="html">Introduction Part of the challenge of backtesting is coming up with a repeatable framework to compare trading systems. In this example, I will use Monte Carlo simulation in R as a way to measure whether or not a particular trading system is better than pure chance.</summary></entry><entry><title type="html">Testing the Bespoke Stock Scores in R</title><link href="http://localhost:4000/r/backtesting/2019/09/11/Testing-the-Bespoke-Stock-Scores-in-R/" rel="alternate" type="text/html" title="Testing the Bespoke Stock Scores in R" /><published>2019-09-11T00:00:00-05:00</published><updated>2019-09-11T00:00:00-05:00</updated><id>http://localhost:4000/r/backtesting/2019/09/11/Testing-the-Bespoke-Stock-Scores-in-R</id><content type="html" xml:base="http://localhost:4000/r/backtesting/2019/09/11/Testing-the-Bespoke-Stock-Scores-in-R/">&lt;p&gt;I have been a subscriber to &lt;a href=&quot;https://www.bespokepremium.com&quot;&gt;Bespoke Premium&lt;/a&gt; for a long time.  Tuesdays are always a special day; that’s when Bespoke Members get a fresh copy of the Bespoke Stock Scores Report.  This is their proprietary blend of Technical, Fundamental and Sentiment Analysis that is derived down to a single, weighted score.  I have always wanted to backtest their recommendations.  So I decided to do so in R.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;From their website:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The Bespoke Stock Scores are a complete proprietary ranking of a stock’s attractiveness made up of three categories - Fundamental, Technical, and Sentiment.
The Fundamental Score is an overall measure of a stock’s key financial ratios including earnings, sales, growth, and yield.  In addition, we analyze each company’s debt and capital levels and compare these to their peers.
The Technical Score analyzes and measures indicators related to a stock’s momentum, relative strength, trend, and volume.  Each stock is then compared to the market and its group.
The Sentiment Score is derived by analyzing the actions of investors and analysts.  Sub categories include trends in institutional ownership, option activity, short interest, analyst sentiment, and seasonal performance patterns.
The Total Score comprises a weighted average of the three main categories described above.  Our experience has shown that certain environments require different approaches.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A score over 70 indicates that the stock is a good candidate for a buy evaluation.&lt;/p&gt;

&lt;p&gt;They publish all of the scores for all of the S&amp;amp;P 1500 stocks ranked accordingly.  They also publish a list of the Top 40 Stock Scores.  This report comes out weekly, generally on Tuesdays.&lt;/p&gt;

&lt;h2 id=&quot;tools&quot;&gt;Tools&lt;/h2&gt;
&lt;p&gt;I used R for the data analysis and &lt;a href=&quot;www.iqFeed.net&quot;&gt;iQFeed&lt;/a&gt; via the (QCollector Expert)[http://www.mechtrading.com/] for the historical stock prices.&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;The goal of this test was to determine if the recommendations, specifically the Top 40 recommendations, give you a statistical edge over the rest of the stocks in the S&amp;amp;P 1500 and whether adapting a trading system using the Top 40 rankings is a reliable and profitable approach as compared to a “buy and hold” approach of the S&amp;amp;P 1500.&lt;/p&gt;

&lt;h2 id=&quot;characteristics&quot;&gt;Characteristics&lt;/h2&gt;
&lt;p&gt;I looked at 589 Excel files that each contained different lists of stock scores dating back to May 29, 2007.  Notably, I missed a few reports.  That’s as far back as I could find the Bespoke Stock Scores, although they may have some that are older.&lt;/p&gt;

&lt;p&gt;From each file, I extracted the Top 40 stocks and the date they were recommended.  This totaled approximately 23,560 individual stock pick recommendations.  It is important to emphasize that Bespoke isn’t technically recommending these stocks (nor am I).  Rather, a score is a ranking relative to all of the remaining stocks in the S&amp;amp;P 1500.  For the purpose of this report, I am assuming that the stock is purchased at the open on the date it is recommended as a Top 40 stock (&lt;strong&gt;“Entry Date”&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;After all of the files were examined, there were 1587 different symbols – some that are no longer traded and some that have been recommended multiple times.&lt;/p&gt;

&lt;p&gt;One of the challenges that I faced was that while the Bespoke Stock Scores gave me an easy entry rule (ie, enter on the date of the recommendation), there were no exit rules.&lt;/p&gt;

&lt;p&gt;Thus, I came up with a few options to exit my position:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Exit after a given number of calendar days (not trading days) after the recommendation date.&lt;/strong&gt;  I referred to this in the code as &lt;strong&gt;“nDays”&lt;/strong&gt; and set it at 365 days.  I looked at holding the stock for 1 – 365 days after the Entry Date.  Because of the practicalities of reporting, I only show reports for 30, 90, 180 and 365 day holding periods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Exit after some given profit target or stop loss was hit.&lt;/strong&gt;  For a couple of different reasons, I did not attempt to model this.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Exit after the Bespoke Stock Score fell below 70 after entry.&lt;/strong&gt; I discarded this idea because of the time between entry and a (possible) exit was, at a minimum, 1 week.  For example, a stock might get recommended on a Tuesday but by the following day, it might have fallen below 70.  You wouldn’t know this immediately because you have to wait until the following Tuesday for the next report to come out.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Don’t worry about the exit (so to speak) and evaluate each trade on a daily basis, evaluating the daily returns, maximum drawdown and the annualized return compared with the standard deviation of the daily returns (Sharpe Ratio).&lt;/strong&gt; Because nDays was set to 365 (one year) I could compare the annualized daily returns and risk of the system to the annualized risk/return of the benchmark.  Note:  for the purpose of this report, the risk-free rate for the Sharpe Ratio is set to zero.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Long 40 Stocks Weekly.&lt;/strong&gt;  It’s important to note that the way the system is designed for this report, you would be buying 40 stocks each week.  Sometimes a specific stock is recommended multiple weeks in a row.  That means that you could have a concentrated position in one single stock.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Holding Period is Important.&lt;/strong&gt; Under Exit #1 above (exit after nDays), you could potentially be in a position longer than 1 year.  This makes a comparison with other benchmarks a little more difficult.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Entry Values Not Included.&lt;/strong&gt; Since &lt;a href=&quot;www.bespokepremium.com&quot;&gt;Bespoke Premium&lt;/a&gt; is a paid service, I did not include any data related to their specific recommendations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;process&quot;&gt;Process&lt;/h2&gt;
&lt;p&gt;(Note:  if you aren’t interested in R or how to replicate this report, skip down to the next section.)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;I had R create and read in a list of all 1587 stocks that were recommended.  I looked up the historical daily stock prices using &lt;a href=&quot;http://www.mechtrading.com/&quot;&gt;QCollector&lt;/a&gt; and saved those as .txt files of Open/High/Low/Close/Volume going back to January 1, 2007.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I had R combine all of this history of all of the stocks, sorted by the stock symbol. In doing so, I extracted the entry date of the recommendation from the file name.  Each file was saved as some form of “BespokeStockScoresMMDDYY.xls.”  Each “MMDDYY” corresponded to the entry date of the recommendation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I found the entry value of the stock on the date of the recommendation.  Although this is given in each of the Bespoke Top 40 reports, I wanted to make sure that the system was using accurate historical prices (it was).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I cycled through the recommendation lists at entry to find the value at exit from 1 to nDays days after each entry recommendation.  Much like the Entry Date, I assumed that the exit would happen on the Open at the nth day (&lt;strong&gt;“Exit Date”&lt;/strong&gt;).  I then calculated the cumulative percentage and point return gained for each recommendation for 1 - 365 days.  This strategy assumed that you entered &lt;strong&gt;EVERY&lt;/strong&gt; stock on the recommendation date regardless of your existing position.  For example, you would be long 40 different stocks every week.  Over the testing period, it means you would make 23,560 trades.  I will discuss the value of this approach and how it could be improved in the conclusions section below.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I then created a table showing each stock and how many times each stock was recommended.  Below is a list of the Top 20 most recommended stocks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Top_20_Stocks.PNG&quot; alt=&quot;Top 20 Stocks By Times Recommended&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I then separated the cumulative daily percent return for &lt;strong&gt;EACH STOCK&lt;/strong&gt; (rather than by recommendation date, as in Step 4 above).  This return was measured from the Entry Date. This allowed me to analyze the cumulative return of each stock recommendation from 1 to nDays. For example, ABM was recommended 49 times during the time period tested. The first time was on 2010-07-13, and on that date it opened at 21.56.  Below is the cumulative returns for the following 252 trading days.  As you can see, it performed rather well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Profit_Loss_ABM.png&quot; alt=&quot;Example Cumulative Return Graph&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I also separated the daily percent return (day over day returns) for each stock from the date of the recommendation. This helped in evaluating the day-to-day performance of each recommendation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;Initially, it was difficult to measure the performance of the system against the S&amp;amp;P1500 benchmark.  Comparing the cumulative performance against a benchmark proved futile.  However, it was easier comparing the system on a day-to-day basis versus the benchmark.  I attempted to annualize the 30/90/180 day exit performance so as to compare that against the historical annualized performance of the S&amp;amp;P Composite 1500. Additionally, I was only able to obtain information on the S&amp;amp;P Composite 1500 dating back to August 31, 2009.  For that reason, relevant benchmark comparisons will ignore any recommendations prior to this date.&lt;/p&gt;

&lt;h3 id=&quot;exit-after-30-calendar-days-from-entry-date&quot;&gt;Exit After 30 Calendar Days From Entry Date&lt;/h3&gt;

&lt;h4 id=&quot;cumulative-percent-return-with-30-day-exit&quot;&gt;Cumulative Percent Return with 30 Day Exit&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/Profit_Loss_30.png&quot; alt=&quot;Profit &amp;amp; Loss Graph - Exit at Day 30&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exiting at 30 days after recommendation provided an average return of .67%, a winning percentage of 56.12%, a profit factor (Average Win/Average Loss) of -.95 and a cumulative return of 10,838%.&lt;/p&gt;

&lt;h4 id=&quot;benchmark-comparison-with-30-day-exit&quot;&gt;Benchmark Comparison with 30 Day Exit&lt;/h4&gt;
&lt;p&gt;Since August 31, 2009, the 30 day exit strategy provided an annualized return of 12.58% with annualized volatility (risk) of 28.4% for a Sharpe Ratio of 0.44.&lt;/p&gt;

&lt;p&gt;This compares unfavorably versus the S&amp;amp;P Composite 1500 - see the table below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Annualized Return (%)&lt;/th&gt;
      &lt;th&gt;Annualized Risk (%)&lt;/th&gt;
      &lt;th&gt;Sharpe Ratio&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;30 Day Strategy&lt;/td&gt;
      &lt;td&gt;12.58&lt;/td&gt;
      &lt;td&gt;28.49&lt;/td&gt;
      &lt;td&gt;0.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Benchmark&lt;/td&gt;
      &lt;td&gt;13.42&lt;/td&gt;
      &lt;td&gt;12.77&lt;/td&gt;
      &lt;td&gt;1.05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note:  see &lt;a href=&quot;https://www.barclayhedge.com/annualized-standard-deviation-of-monthly-quarterly-return/&quot;&gt;this link&lt;/a&gt; for calculating Annualized Standard Deviation of Monthly / Quarterly Return.&lt;/p&gt;

&lt;h4 id=&quot;cumulative-percent-return-with-90-day-exit&quot;&gt;Cumulative Percent Return with 90 Day Exit&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/Profit_Loss_90.png&quot; alt=&quot;Profit &amp;amp; Loss Graph - Exit at Day 90&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exiting at 90 days after recommendation provided an average return of 1.2%, a winning percentage of 55.7%, a profit factor (Average Win/Average Loss) of -.97 and a cumulative return of 17,646%.&lt;/p&gt;

&lt;h4 id=&quot;benchmark-comparison-with-90-day-exit&quot;&gt;Benchmark Comparison with 90 Day Exit&lt;/h4&gt;
&lt;p&gt;Since August 31, 2009, the 90 day exit strategy provided an annualized return of 9.8% with annualized volatility (risk) of 28.31% for a Sharpe Ratio of .35.&lt;/p&gt;

&lt;p&gt;This is not as favorable to the benchmark - see table below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Annualized Return (%)&lt;/th&gt;
      &lt;th&gt;Annualized Risk (%)&lt;/th&gt;
      &lt;th&gt;Sharpe Ratio&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;90 Day Strategy&lt;/td&gt;
      &lt;td&gt;9.8&lt;/td&gt;
      &lt;td&gt;28.31&lt;/td&gt;
      &lt;td&gt;0.35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Benchmark&lt;/td&gt;
      &lt;td&gt;13.42&lt;/td&gt;
      &lt;td&gt;12.77&lt;/td&gt;
      &lt;td&gt;1.05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;cumulative-percent-return-with-180-day-exit&quot;&gt;Cumulative Percent Return with 180 Day Exit&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/Profit_Loss_180.png&quot; alt=&quot;Profit &amp;amp; Loss Graph - Exit at Day 180&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exiting at 180 days after recommendation provided an average return of 2.8%, a winning percentage of 56.8%, a profit factor (Average Win/Average Loss) of -1.05 and a cumulative return of 39,545%.&lt;/p&gt;

&lt;h4 id=&quot;benchmark-comparison-with-180-day-exit&quot;&gt;Benchmark Comparison with 180 Day Exit&lt;/h4&gt;
&lt;p&gt;Since August 31, 2009, the 180 day exit strategy provided an annualized return of 10.17% with annualized volatility (risk) of 28.4% for a Sharpe Ratio of .35.&lt;/p&gt;

&lt;p&gt;This is not as favorable to the benchmark - see table below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Annualized Return (%)&lt;/th&gt;
      &lt;th&gt;Annualized Risk (%)&lt;/th&gt;
      &lt;th&gt;Sharpe Ratio&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;180 Day Strategy&lt;/td&gt;
      &lt;td&gt;10.17&lt;/td&gt;
      &lt;td&gt;28.4&lt;/td&gt;
      &lt;td&gt;0.35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Benchmark&lt;/td&gt;
      &lt;td&gt;13.42&lt;/td&gt;
      &lt;td&gt;12.77&lt;/td&gt;
      &lt;td&gt;1.05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;cumulative-percent-return-with-365-day-exit&quot;&gt;Cumulative Percent Return with 365 Day Exit&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/Profit_Loss_365.png&quot; alt=&quot;Profit &amp;amp; Loss Graph - Exit at Day 365&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exiting at 365 days after recommendation provided an average return of 8.3%, a winning percentage of 61.3%, a profit factor (Average Win/Average Loss) of -1.24 and a cumulative return of 116,981%.&lt;/p&gt;

&lt;h4 id=&quot;benchmark-comparison-with-365-day-exit&quot;&gt;Benchmark Comparison with 365 Day Exit&lt;/h4&gt;
&lt;p&gt;Since August 31, 2009, the 365 day exit strategy provided an annualized return of 11.9% with annualized volatility (risk) of 29.45% for a Sharpe Ratio of .40.&lt;/p&gt;

&lt;p&gt;This is not as favorable to the benchmark - see table below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Annualized Return (%)&lt;/th&gt;
      &lt;th&gt;Annualized Risk (%)&lt;/th&gt;
      &lt;th&gt;Sharpe Ratio&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;365 Day Strategy&lt;/td&gt;
      &lt;td&gt;11.9&lt;/td&gt;
      &lt;td&gt;29.45&lt;/td&gt;
      &lt;td&gt;.406&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Benchmark&lt;/td&gt;
      &lt;td&gt;13.42&lt;/td&gt;
      &lt;td&gt;12.77&lt;/td&gt;
      &lt;td&gt;1.05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Over each time period, the strategy had a lower risk-adjusted return as compared to the S&amp;amp;P Composite 1500 benchmark.  However, over the short-term, the Top 40 Bespoke Stock Scores significantly outperformed the benchmark on a pure, risk-unadjusted annualized return basis.  For instance, exiting at 10 days provided a 17.24% annualized return, exceeding the benchmark by more than 4 points.&lt;/p&gt;

&lt;h2 id=&quot;notes--additional-research&quot;&gt;Notes &amp;amp; Additional Research&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;See this excellent blog post on evaluating trading strategies in R.  &lt;a href=&quot;https://www.signalplot.com/how-to-measure-the-performance-of-a-trading-strategy/&quot;&gt;How to Measure the Performance of a Trading Strategy&lt;/a&gt;.  The author also includes a link to the R Code here:  &lt;a href=&quot;https://github.com/luyongxu/SignalPlot/blob/master/1.001%20Code/1.005%20Measuring%20Performance.R&quot;&gt;1.005 Measuring Performance.R&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The graphs on this report were created using the &lt;a href=&quot;https://bbc.github.io/rcookbook/&quot;&gt;BBC Visual and Data Journalism cookbook for R graphics&lt;/a&gt;.  You can find all of the R code to recreate the BBC style graphics at the &lt;a href=&quot;https://github.com/bbc/bbplot&quot;&gt;BBC Github Repository&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;S&amp;amp;P 1500 historical performance was obtained from the &lt;a href=&quot;https://us.spindices.com/indices/equity/sp-composite-1500&quot;&gt;S&amp;amp;P Composite 1500 Page&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Annualized Return is the constant annual return applied to each period in arrays that would result in the actual compounded return over that range. An Annualized Return is a special case of a Geometric Average Return where the time periods are expressed in terms of years.” See &lt;a href=&quot;http://www.crsp.com/products/documentation/crsp-calculations&quot;&gt;CRSP Calculations&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Jake Stoetzner</name><email>Jake.stoetzner@gmail.com</email></author><category term="r" /><category term="backtesting" /><category term="Bespoke" /><summary type="html">I have been a subscriber to Bespoke Premium for a long time. Tuesdays are always a special day; that’s when Bespoke Members get a fresh copy of the Bespoke Stock Scores Report. This is their proprietary blend of Technical, Fundamental and Sentiment Analysis that is derived down to a single, weighted score. I have always wanted to backtest their recommendations. So I decided to do so in R.</summary></entry></feed>